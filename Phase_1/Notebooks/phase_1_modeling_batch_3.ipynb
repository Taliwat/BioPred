{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Phase 1 Modeling**\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "Now that we have our feature set that we want, we will dive into modeling for this first phase, in which we are predicting the min_standard_value for bioactivity of our molecules.  This will be very foundational for the rest of the project.\n",
    "\n",
    "In this notebook we will examine our ph1_df dataframe, and model it with XGBoost, RandomForest and SVR; hypertuning with BayesOpt and then looking at R2 and MSE for metrics before plotting the residuals.  We will conduct 3 different models in order to compare them before looking at a metrics table to view overall results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just as we did in our other notebook formats, let's read in our data for this notebook as well as the appropriate libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in libraries needed.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our subject df for this notebook.\n",
    "file_path = \"/home/azureuser/cloudfiles/code/Users/kalpha1865/BioPred/Data/df_files/modeling_phase_1.parquet\"\n",
    "ph1_df = pd.read_parquet(file_path)\n",
    "\n",
    "print(ph1_df.head())\n",
    "print(ph1_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we begin we will need to formally convert our Morgan Fingerprints into NumPy arrays.  They are still in ExplicitBitVect form, which are unusable at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate and convert to arrays\n",
    "X_fingerprints = np.vstack(ph1_df['morgan_fingerprints'].apply(lambda fp: np.array(list(fp), dtype = np.uint8)))\n",
    "\n",
    "# Drop the original column from the df\n",
    "ph1_df.drop(columns = ['morgan_fingerprints'], inplace = True)\n",
    "\n",
    "# confirm shape\n",
    "print(f\"Fingerprint matrix shape: {X_fingerprints.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great we can see that the shape is being represented as 2D, with the correct rows and 2048 bits.  Good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's set up our models that we will use with our X,y and our train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,y variables\n",
    "X_numerical = ph1_df.drop(columns = ['min_standard_value']) # Drop target\n",
    "X = np.hstack([X_fingerprints, X_numerical.values]) # Add the Morgan Fingerprints back in\n",
    "y = ph1_df['min_standard_value'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_test_split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will run all 3 of our subject models, and using BayesOpt on each of them.  We will look at the MSE and R2 metrics for each of them and compare, as well as plotting residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models for the third batch\n",
    "models_tre = {\n",
    "    'HistGradientBoosting': HistGradientBoostingRegressor(\n",
    "        loss='squared_error',\n",
    "        learning_rate=0.1,\n",
    "        max_iter=100,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=20,\n",
    "        max_bins=255,\n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    'ExtraTrees': ExtraTreesRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    'GradientBoosting': GradientBoostingRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model runs and evaluation\n",
    "results_initial_tre = {}\n",
    "\n",
    "for name, model in models_tre.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results_initial_tre[name] = {'MSE' : mse, 'R2' : r2}\n",
    "    print(f\"{name} - MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    # Predicted vs Actual plots\n",
    "    plt.figure(figsize = (10,8))\n",
    "    plt.scatter(y_test, y_pred, alpha = 0.5)\n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(f\"Predicted vs Actual - {name}\")\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.show()\n",
    "    \n",
    "    # Histogram of Residuals\n",
    "    plt.figure(figsize = (10,8))\n",
    "    plt.hist(y_test - y_pred, bins = 30, alpha = 0.7, edgecolor = 'black')\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Histogram of Residuals - {name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter spaces for BayesOpt\n",
    "search_spaces_tre = {\n",
    "    'HistGradientBoosting': {\n",
    "        'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
    "        'max_iter': (100, 500),\n",
    "        'max_depth': (3, 20),\n",
    "        'min_samples_leaf': (5, 50),\n",
    "        'max_bins': (128, 512)\n",
    "    },\n",
    "    'ExtraTrees': {\n",
    "        'n_estimators': (100, 500),\n",
    "        'max_depth': (5, 50),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'min_samples_leaf': (1, 10),\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
    "        'n_estimators': (100, 500),\n",
    "        'max_depth': (3, 20),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'min_samples_leaf': (1, 10),\n",
    "        'subsample': (0.5, 1.0)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Bayes Optimization\n",
    "optimized_models_tre = {}\n",
    "\n",
    "for name, model in models_tre.items():\n",
    "    print(f\"Optimizing {name}...\")\n",
    "    opt = BayesSearchCV(\n",
    "        model, search_spaces_tre[name], n_iter = 20, cv = 3, scoring = 'r2', n_jobs = 2, random_state = 42\n",
    "    )\n",
    "    opt.fit(X_train, y_train)\n",
    "    optimized_models_tre[name] = opt.best_estimator_\n",
    "    print(f\"Best params for {name}: {opt.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models\n",
    "results_optimized_tre = {}\n",
    "\n",
    "for name, model in optimized_models_tre.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results_optimized_tre[name] = {'MSE' : mse, 'R2' : r2}\n",
    "\n",
    "    # Plot residuals for each model\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    plt.scatter(y_test, y_test - y_pred, alpha = 0.5)\n",
    "    plt.axhline(y = 0, color = 'r', linestyle = '--')\n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(f\"Residuals Plot - {name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature Importances for the Tree-Based Models\n",
    "    if name in ['HistGradientBoosting', 'ExtraTrees', 'GradientBoosting']:\n",
    "        importances = model.feature_importances_\n",
    "        sorted_idx = np.argsort(importances)\n",
    "        plt.figure(figsize = (10, 8))\n",
    "        plt.barh(X.columns[sorted_idx], importances[sorted_idx], color = 'orange')\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.title(f\"Feature Importance - {name}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Results\n",
    "results_initial_df_tre = pd.DataFrame(results_initial_tre).T\n",
    "results_optimized_df_tre = pd.DataFrame(results_optimized_tre).T\n",
    "\n",
    "print(\"\\nInitial Model Results:\")\n",
    "print(results_initial_df_tre)\n",
    "print(\"\\nOptimized Model Results:\")\n",
    "print(results_optimized_df_tre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
