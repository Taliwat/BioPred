{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **ChEMBL Data Import and Preparation**\n",
        "\n",
        "## Objectives:\n",
        "\n",
        "The first notebook focuses on setting up the data pipeline by:\n",
        "\n",
        "1.  Reading the ChEMBL dataset directly into our Azure MySQL Database from the ChEMBL URL.\n",
        "\n",
        "2.  Taking the new raw data, querying it for what we need at this time, and placing it in a separate database.\n",
        "\n",
        "3.  Examines this new set of data for duplicates and missing values, removing as needed.\n",
        "\n",
        "4.  Saving our work so that we can use this version of the data for the rest of this phase of the project, saving also as a parquet file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 1: Import Libraries and Establish Project Root for Directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### First let's set our directory to the root of the project.  Doing so will keep our project on track location-wise and it is a great way to keep yourself out of trouble with your directory issues, by setting your abspath to the root.  We can then in future notebooks refer back to the project_root when needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root set to: /mnt/batch/tasks/shared/LS_root/mounts/clusters/kalpha18651/code/Users/kalpha1865/BioPred\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define project root\n",
        "project_root = \"/home/azureuser/cloudfiles/code/Users/kalpha1865/BioPred\"\n",
        "\n",
        "# Validate the directory\n",
        "if not os.path.exists(project_root):\n",
        "    raise FileNotFoundError(f\"Project root not found: {project_root}\")\n",
        "\n",
        "# Change working directory to project root if not already\n",
        "if os.getcwd() != project_root:\n",
        "    os.chdir(project_root)\n",
        "\n",
        "print(f\"Project root set to: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Now we can import the rest of our libraries, as well as establish a reference point to our Config file for our database credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1735953271559
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config directory: /home/azureuser/cloudfiles/code/Users/kalpha1865/BioPred/Config\n",
            "Files in Config directory: ['config.py', '__pycache__']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import requests\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import fastparquet\n",
        "from sqlalchemy import create_engine, text, Index, MetaData, Table, inspect\n",
        "import mysql.connector\n",
        "\n",
        "\n",
        "# Referencing the config file for Azure MySQL Database credentials.\n",
        "config_dir = os.path.join(project_root, \"Config\")\n",
        "sys.path.append(config_dir)\n",
        "print(f\"Config directory: {config_dir}\")\n",
        "print(\"Files in Config directory:\", os.listdir(config_dir))\n",
        "\n",
        "from config import MYSQL_CONFIG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 2: Read and Extract Data from URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Now we will bring in our ChEMBL data, sending it straight from the site url to our Azure MySQL Database.  We will read in our credentials from a config file for access.  The commands used in the function below can normally be used in the terminal however I wanted to show my work here.  First though we will need to create our databases that we will use in this portion of the project to house and work with the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database 'chembl_raw' created successfully.\n",
            "Database 'chembl_phase_1' created successfully.\n",
            "Verification successful: Databases exist in the Azure MySQL server.\n"
          ]
        }
      ],
      "source": [
        "# setting up as a try/except block so we can add error handling.\n",
        "try:\n",
        "    # Set up the connection string to Azure\n",
        "    engine = create_engine(\n",
        "        f\"mysql+mysqlconnector://{MYSQL_CONFIG['username']}:{MYSQL_CONFIG['password']}@\"\n",
        "        f\"{MYSQL_CONFIG['hostname']}:{MYSQL_CONFIG['port']}/\",\n",
        "        connect_args={\n",
        "            \"ssl_ca\" : MYSQL_CONFIG[\"ssl_ca\"],\n",
        "            \"ssl_verify_cert\" : True\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Create the databases needed\n",
        "    with engine.connect() as connection:\n",
        "        connection.execute(text(\"CREATE DATABASE IF NOT EXISTS chembl_raw;\"))\n",
        "        print(\"Database 'chembl_raw' created successfully.\")\n",
        "    \n",
        "        connection.execute(text(\"CREATE DATABASE IF NOT EXISTS chembl_phase_1;\"))\n",
        "        print(\"Database 'chembl_phase_1' created successfully.\")\n",
        "    \n",
        "        # Verify databases exist.\n",
        "        result = connection.execute(text(\"SHOW DATABASES;\"))\n",
        "        databases = [row[0] for row in result]\n",
        "        \n",
        "        if \"chembl_raw\" in databases and \"chembl_phase_1\" in databases:\n",
        "            print(\"Verification successful: Databases exist in the Azure MySQL server.\")\n",
        "        else:\n",
        "            print(\"Error: Databases were not found after creation.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Great the new databases are created and housed in our Azure MySQL server (I verified on the Azure portal as well).  Now let's fetch the url containing our data from ChEMBL and send it to our empty database, so we can query off of it and get what we need for phase_1 data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting tar file...\n",
            "Loading .dmp file into MySQL...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "mysql: [Warning] Using a password on the command line interface can be insecure.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data successfully loaded into MySQL.\n"
          ]
        }
      ],
      "source": [
        "# Function to fetch, extract, and send the contents to our db.\n",
        "\n",
        "def prepare_and_load_data():\n",
        "    tar_file = \"/home/azureuser/cloudfiles/code/Users/kalpha1865/BioPred/chembl_35_mysql.tar.gz\"\n",
        "    extract_dir = \"/home/azureuser/cloudfiles/code/Users/kalpha1865/BioPred/chembl_35/chembl_35_mysql/\"\n",
        "    dmp_file = os.path.join(extract_dir, \"chembl_35_mysql.dmp\")\n",
        "\n",
        "    # Verify tar file\n",
        "    if not os.path.exists(tar_file):\n",
        "        print(f\"{tar_file} not found. Downloading...\")\n",
        "        subprocess.run(\n",
        "            f\"wget ftp://ftp.ebi.ac.uk/pub/databases/chembl/ChEMBLdb/latest/chembl_35_mysql.tar.gz -O {tar_file}\",\n",
        "            shell=True\n",
        "        )\n",
        "    \n",
        "    # Clean up existing files\n",
        "    if os.path.exists(extract_dir):\n",
        "        print(f\"Removing existing directory: {extract_dir}\")\n",
        "        subprocess.run(f\"rm -rf {extract_dir}\", shell=True)\n",
        "    \n",
        "    # Extract tar file\n",
        "    print(\"Extracting tar file...\")\n",
        "    process = subprocess.run(f\"tar -xzf {tar_file} -C /home/azureuser/cloudfiles/code/Users/kalpha1865/BioPred/\", shell=True)\n",
        "    if process.returncode != 0:\n",
        "        print(\"Error during extraction.\")\n",
        "        return\n",
        "    \n",
        "    # Verify .dmp file\n",
        "    if not os.path.exists(dmp_file):\n",
        "        print(f\"{dmp_file} not found after extraction.\")\n",
        "        return\n",
        "    \n",
        "    print(\"Loading .dmp file into MySQL...\")\n",
        "    load_command = (\n",
        "        f\"mysql -h biopred.mysql.database.azure.com -u rdm1 -p'tali1327_yo' \"\n",
        "        f\"--ssl-mode=VERIFY_CA --ssl-ca=/home/azureuser/cloudfiles/code/Users/kalpha1865/BioPred/docs/certs/DigiCertGlobalRootCA.crt.pem \"\n",
        "        f\"-D chembl_raw < {dmp_file}\"\n",
        "    )\n",
        "    process = subprocess.run(load_command, shell=True)\n",
        "    if process.returncode == 0:\n",
        "        print(\"Data successfully loaded into MySQL.\")\n",
        "    else:\n",
        "        print(f\"Error loading data. Return code: {process.returncode}\")\n",
        "\n",
        "prepare_and_load_data()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Now that we have the data sent to our database let's create a new connection and check the table names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tables in raw database: ['action_type', 'activities', 'activity_properties', 'activity_smid', 'activity_stds_lookup', 'activity_supp', 'activity_supp_map', 'assay_class_map', 'assay_classification', 'assay_parameters', 'assay_type', 'assays', 'atc_classification', 'binding_sites', 'bio_component_sequences', 'bioassay_ontology', 'biotherapeutic_components', 'biotherapeutics', 'cell_dictionary', 'chembl_id_lookup', 'chembl_release', 'component_class', 'component_domains', 'component_go', 'component_sequences', 'component_synonyms', 'compound_properties', 'compound_records', 'compound_structural_alerts', 'compound_structures', 'confidence_score_lookup', 'curation_lookup', 'data_validity_lookup', 'defined_daily_dose', 'docs', 'domains', 'drug_indication', 'drug_mechanism', 'drug_warning', 'formulations', 'frac_classification', 'go_classification', 'hrac_classification', 'indication_refs', 'irac_classification', 'ligand_eff', 'mechanism_refs', 'metabolism', 'metabolism_refs', 'molecule_atc_classification', 'molecule_dictionary', 'molecule_frac_classification', 'molecule_hierarchy', 'molecule_hrac_classification', 'molecule_irac_classification', 'molecule_synonyms', 'organism_class', 'patent_use_codes', 'predicted_binding_domains', 'product_patents', 'products', 'protein_class_synonyms', 'protein_classification', 'relationship_type', 'research_companies', 'research_stem', 'site_components', 'source', 'structural_alert_sets', 'structural_alerts', 'target_components', 'target_dictionary', 'target_relations', 'target_type', 'tissue_dictionary', 'usan_stems', 'variant_sequences', 'version', 'warning_refs']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define a new engine for the connection, specifying our new database for the raw data.\n",
        "raw_engine = create_engine(\n",
        "        f\"mysql+mysqlconnector://{MYSQL_CONFIG['username']}:{MYSQL_CONFIG['password']}@\"\n",
        "        f\"{MYSQL_CONFIG['hostname']}:{MYSQL_CONFIG['port']}/{MYSQL_CONFIG['database_raw']}\",\n",
        "        connect_args={\n",
        "            \"ssl_ca\" : MYSQL_CONFIG[\"ssl_ca\"],\n",
        "            \"ssl_verify_cert\" : True\n",
        "        }\n",
        "    )\n",
        "\n",
        "with raw_engine.connect() as connection:\n",
        "    result = connection.execute(text(\"SHOW TABLES;\"))\n",
        "    print(\"Tables in raw database:\", [row[0] for row in result.fetchall()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 3: Querying Our New Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Let's take a look at the schema to get a visual for the ChEMBL data. Seen below.  We will use this to formulate our query and our indexes for the next part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://ftp.ebi.ac.uk/pub/databases/chembl/ChEMBLdb/latest/chembl_35_schema.png\" alt = \"ChEMBL Schema\" width = 2000>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Next we will set a few indexes for our data, this will help considerably when we go to query our data.  We will also set indexes with a general theme, setting indexes for features we will need and use throughout this project so we don't change them again as this is the raw data we will be iterating on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Before creating our indexes though we need to map the MetaData of our tables that we will need for our joins in our query.  This will allow us to query a lot faster through our forthcoming indexes, as with those we won't need to scan the whole dataset every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata = MetaData()\n",
        "\n",
        "metadata.reflect(bind=raw_engine)\n",
        "\n",
        "# Reflect the tables to the metadata.\n",
        "compound_structures = Table(\"compound_structures\", metadata, autoload_with=raw_engine)\n",
        "activities = Table(\"activities\", metadata, autoload_with=raw_engine)\n",
        "assays = Table(\"assays\", metadata, autoload_with=raw_engine)\n",
        "target_dictionary = Table(\"target_dictionary\", metadata, autoload_with=raw_engine)\n",
        "compound_properties = Table(\"compound_properties\", metadata, autoload_with=raw_engine)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### And a check to see existing indexes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'name': 'compound_structures_pk', 'column_names': ['molregno'], 'unique': True, 'type': 'UNIQUE'}, {'name': 'idx_cmpdstr_stdkey', 'column_names': ['standard_inchi_key'], 'unique': False}, {'name': 'ix_molregno', 'column_names': ['molregno'], 'unique': False}, {'name': 'uk_cmpdstr_stdinchkey', 'column_names': ['standard_inchi_key'], 'unique': True, 'type': 'UNIQUE'}]\n"
          ]
        }
      ],
      "source": [
        "inspector = inspect(raw_engine)\n",
        "indexes = inspector.get_indexes(\"compound_structures\") # Example table\n",
        "print(indexes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Now let's create a function to add our new indexes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index ix_molregno already exists. Skipping creation.\n",
            "Index ix_activities_molregno already exists. Skipping creation.\n",
            "Index ix_assay_id already exists. Skipping creation.\n",
            "Index ix_tid already exists. Skipping creation.\n",
            "Index ix_full_mwt already exists. Skipping creation.\n",
            "Index ix_hba_lipinski already exists. Skipping creation.\n",
            "Index ix_hbd_lipinski already exists. Skipping creation.\n",
            "Index ix_alogp already exists. Skipping creation.\n",
            "Index ix_psa already exists. Skipping creation.\n",
            "Index ix_rtb already exists. Skipping creation.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate function to create new index if it doesn't exist\n",
        "def create_index(table_name, index_name, column_name):\n",
        "    inspector = inspect(raw_engine)\n",
        "    existing_indexes = [idx[\"name\"] for idx in inspector.get_indexes(table_name)]\n",
        "    if index_name not in existing_indexes:\n",
        "        Index(index_name, column_name).create(raw_engine)\n",
        "        print(f\"Index {index_name} created successfully.\")\n",
        "    else:\n",
        "        print(f\"Index {index_name} already exists. Skipping creation.\")\n",
        "\n",
        "# Use function to add new wanted indexes\n",
        "\n",
        "# First the indexes for the joins\n",
        "create_index(\"compound_structures\", \"ix_molregno\", compound_structures.c.molregno)\n",
        "create_index(\"activities\", \"ix_activities_molregno\", activities.c.molregno)\n",
        "create_index(\"assays\", \"ix_assay_id\", assays.c.assay_id)\n",
        "create_index(\"target_dictionary\", \"ix_tid\", target_dictionary.c.tid)\n",
        "\n",
        "# Now the indexes for filtering\n",
        "create_index(\"compound_properties\", \"ix_full_mwt\", compound_properties.c.full_mwt)\n",
        "create_index(\"compound_properties\", \"ix_hba_lipinski\", compound_properties.c.hba_lipinski)\n",
        "create_index(\"compound_properties\", \"ix_hbd_lipinski\", compound_properties.c.hbd_lipinski)\n",
        "create_index(\"compound_properties\", \"ix_alogp\", compound_properties.c.alogp)\n",
        "create_index(\"compound_properties\", \"ix_psa\", compound_properties.c.psa)\n",
        "create_index(\"compound_properties\", \"ix_rtb\", compound_properties.c.rtb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### We will first attempt to run the whole query as it is first, then slowly editing and changing as we see results based on duplicate and missing value numbers.  The query below is our final product after numerous iterations, as we relaxed our parameters from Lipinski's Rule of Five set to allow for an increase in data allotment and made sure that our pref_name feature was intact without pulling in a lot of missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query returned dataset shape: (606673, 17)\n",
            "Checking for duplicates...\n",
            "Number of duplicate canonical_smiles: 261729\n",
            "\n",
            "Checking for missing values...\n",
            "canonical_smiles         0\n",
            "standard_inchi_key       0\n",
            "molregno                 0\n",
            "full_mwt                 0\n",
            "alogp                    0\n",
            "psa                      0\n",
            "hba_lipinski             0\n",
            "hbd_lipinski             0\n",
            "aromatic_rings           0\n",
            "heavy_atoms              0\n",
            "rtb                      0\n",
            "molecular_species     2304\n",
            "min_standard_value       0\n",
            "activity_count           0\n",
            "pref_name                0\n",
            "tid                      0\n",
            "target_type              0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Query the raw data to get what we need in our phase_1_db.\n",
        "def query_phase_1_data():\n",
        "    # Targeted query for phase 1 data with subqueries for better processing\n",
        "    query = text(f\"\"\"           \n",
        "    SELECT DISTINCT cs.canonical_smiles, cs.standard_inchi_key, cs.molregno,\n",
        "        MIN(cp.full_mwt) AS full_mwt, MIN(cp.alogp) AS alogp,\n",
        "        MIN(cp.psa) AS psa, MIN(cp.hba_lipinski) AS hba_lipinski, MIN(cp.hbd_lipinski) AS hbd_lipinski,\n",
        "        MIN(cp.aromatic_rings) AS aromatic_rings, MIN(cp.heavy_atoms) AS heavy_atoms,\n",
        "        MIN(cp.rtb) AS rtb, cp.molecular_species, fa.min_standard_value, fa.activity_count,\n",
        "        td.pref_name, td.tid, td.target_type\n",
        "    FROM compound_structures cs\n",
        "    LEFT JOIN compound_properties cp ON cs.molregno = cp.molregno\n",
        "    LEFT JOIN (\n",
        "        SELECT molregno, MIN(standard_value) AS min_standard_value, COUNT(doc_id) AS activity_count, assay_id\n",
        "        FROM activities\n",
        "        WHERE standard_value BETWEEN 0 AND 150\n",
        "            AND standard_type IN ('IC50', 'EC50')\n",
        "        GROUP BY molregno, assay_id\n",
        "    ) fa ON cs.molregno = fa.molregno\n",
        "    LEFT JOIN assays ass ON fa.assay_id = ass.assay_id\n",
        "    LEFT JOIN target_dictionary td ON ass.tid = td.tid\n",
        "    WHERE\n",
        "        cp.full_mwt BETWEEN 100 AND 600\n",
        "        AND cp.alogp BETWEEN -1 AND 6\n",
        "        AND cp.psa <= 180\n",
        "        AND cp.rtb <= 15\n",
        "        AND cp.hbd_lipinski <= 7\n",
        "        AND cp.hba_lipinski <= 15\n",
        "        AND td.pref_name IS NOT NULL\n",
        "    GROUP BY\n",
        "        cs.canonical_smiles,\n",
        "        cs.standard_inchi_key,\n",
        "        cp.molecular_species,\n",
        "        td.pref_name,\n",
        "        td.tid,\n",
        "        td.target_type,\n",
        "        fa.min_standard_value,\n",
        "        fa.activity_count;\n",
        "    \"\"\")\n",
        "\n",
        "\n",
        "    # Execute query with error handling\n",
        "    try:\n",
        "        # Execute query and fetch results\n",
        "        with raw_engine.connect() as connection:\n",
        "            result = connection.execute(query) \n",
        "            df_phase_1 = pd.DataFrame(result.fetchall(), columns = result.keys())\n",
        "            print(f\"Query returned dataset shape: {df_phase_1.shape}\")\n",
        "            return df_phase_1\n",
        "\n",
        "    except Exception as e:\n",
        "            print(f\"Error querying phase 1 data: {e}\")\n",
        "            return pd.DataFrame()\n",
        "    \n",
        "\n",
        "# Check the data for duplicates and missing values before saving.\n",
        "def check_data_quality(df):\n",
        "    if df.empty:\n",
        "        print(\"DataFrame is empty.  Skipping quality checks.\")\n",
        "        return\n",
        "    \n",
        "    print(\"Checking for duplicates...\")\n",
        "    print(f\"Number of duplicate canonical_smiles: {df['canonical_smiles'].duplicated().sum()}\")\n",
        "    \n",
        "    print(\"\\nChecking for missing values...\")\n",
        "    print(df.isna().sum())\n",
        "\n",
        "# Now run both functions\n",
        "df_phase_1 = query_phase_1_data()\n",
        "check_data_quality(df_phase_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Now we can remove the duplicates and the small amount of rows in molecular_species.  We don't want any duplicated data in canonical_smiles as that is going to be our target feature, and the amount of missing values in molecular_species is negligible (0.4%) so there isn't much reason to look into that at this time and removing them to have a clean df is optimal at this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(343659, 17)\n",
            "                                    canonical_smiles  \\\n",
            "0                   O=C1O/C(=C/Br)CCC1c1cccc2ccccc12   \n",
            "1                    O=C1O/C(=C/I)CCC1c1cccc2ccccc12   \n",
            "2                C#C/C=C1\\CCC(c2cccc3ccccc23)C(=O)O1   \n",
            "3  COc1cc2nc(N3CCN(C(=O)C4CC4c4ccccc4)CC3)nc(N)c2...   \n",
            "4  COc1cc2nc(N3CCN(S(=O)(=O)c4cccs4)CC3)nc(N)c2cc1OC   \n",
            "\n",
            "            standard_inchi_key  molregno full_mwt alogp     psa  hba_lipinski  \\\n",
            "0  BYUCSFWXCMTYOI-ZRDIBKRKSA-N        23   317.18  4.50   26.30             2   \n",
            "1  ZJGPRRLHNMNINO-ZRDIBKRKSA-N        24   364.18  4.54   26.30             2   \n",
            "2  NHOIHEBFAKOZIE-MKMNVTDBSA-N        25   262.31  3.78   26.30             2   \n",
            "3  QSRCXSDOJVDQBI-UHFFFAOYSA-N        31   433.51  2.68   93.81             8   \n",
            "4  RYVAEGMROPOPOW-UHFFFAOYSA-N        33   435.53  1.80  110.88             9   \n",
            "\n",
            "   hbd_lipinski  aromatic_rings  heavy_atoms  rtb molecular_species  \\\n",
            "0             0               2           19    1           NEUTRAL   \n",
            "1             0               2           19    1           NEUTRAL   \n",
            "2             0               2           20    1           NEUTRAL   \n",
            "3             2               3           32    5           NEUTRAL   \n",
            "4             2               3           29    5           NEUTRAL   \n",
            "\n",
            "                   min_standard_value  activity_count  \\\n",
            "0  100.000000000000000000000000000000               2   \n",
            "1   30.000000000000000000000000000000               1   \n",
            "2   95.000000000000000000000000000000               1   \n",
            "3    0.512900000000000000000000000000               1   \n",
            "4   33.880000000000000000000000000000               1   \n",
            "\n",
            "                              pref_name     tid     target_type  \n",
            "0  Calcium-independent phospholipase A2   11934  SINGLE PROTEIN  \n",
            "1  Calcium-independent phospholipase A2   11934  SINGLE PROTEIN  \n",
            "2  Calcium-independent phospholipase A2   11934  SINGLE PROTEIN  \n",
            "3           Adrenergic receptor alpha-1  104304  PROTEIN FAMILY  \n",
            "4           Adrenergic receptor alpha-1  104304  PROTEIN FAMILY  \n"
          ]
        }
      ],
      "source": [
        "df_phase_1 = df_phase_1.drop_duplicates(subset = \"canonical_smiles\")\n",
        "df_phase_1 = df_phase_1.dropna(subset = ['molecular_species'])\n",
        "print(df_phase_1.shape)\n",
        "print(df_phase_1.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Just around ~343k rows after the query and clean.  I am okay with this as I am opting for quality data for this phase of the project and it will help our modeling.  We also will have a lot of feature engineering and feature formatting and manipulation to do so this data will be expanding.  Let's save it and finish up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### First we need to make another connection engine for phase_1_data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phase 1 database engine created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Define a new engine for the connection, specifying our new database for the phase_! data.\n",
        "phase_1_engine = create_engine(\n",
        "        f\"mysql+mysqlconnector://{MYSQL_CONFIG['username']}:{MYSQL_CONFIG['password']}@\"\n",
        "        f\"{MYSQL_CONFIG['hostname']}:{MYSQL_CONFIG['port']}/{MYSQL_CONFIG['database_phase_1']}\",\n",
        "        connect_args={\n",
        "            \"ssl_ca\" : MYSQL_CONFIG[\"ssl_ca\"],\n",
        "            \"ssl_verify_cert\" : True\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Phase 1 database engine created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results successfully saved to phase_1_data\n"
          ]
        }
      ],
      "source": [
        "# Try/except block here to save to the MySQL database\n",
        "try:\n",
        "    df_phase_1.to_sql(\n",
        "        name=\"df_phase_1_data\",\n",
        "            con=phase_1_engine,\n",
        "        if_exists=\"replace\",\n",
        "        index = False,\n",
        "        chunksize= 10000        \n",
        "    )\n",
        "    print(f\"Results successfully saved to phase_1_data\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving to target database: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### We will also save as a parquet file so we can carry over and use in our EDA notebook next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Queried phase 1 data saved as Parquet in df_files in Data folder.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "file_path = \"/home/azureuser/cloudfiles/code/Users/kalpha1865/BioPred/Data/df_files/df_phase_1.parquet\"\n",
        "df_phase_1.to_parquet(file_path, index = False)\n",
        "print(\"Queried phase 1 data saved as Parquet in df_files in Data folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### As a final step we will dispose of our connections to our engine(s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All database connections have been closed.\n"
          ]
        }
      ],
      "source": [
        "raw_engine.dispose()\n",
        "phase_1_engine.dispose()\n",
        "\n",
        "print(\"All database connections have been closed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 3: Data Exploration and Filtering\n",
        "\n",
        "SQLite is used to inspect the database schema and query relationships for efficient preparation of data for downstream processes in PySpark.  To avoid unnecessary overhead and ensure efficient data handling, the molfile column from the compound_structures table will be excluded from modeling workflows.  Instead, canonical_smiles will serve as the primary representation for molecular structures, as it is compact and fully compatible with RDKit and GNN workflows.\n",
        "\n",
        "Here are the tables we will be primarily interested in storing at this phase of the project:\n",
        "\n",
        "**compound_structures**:\n",
        "Contains molecule identifiers (SMILES, InChI) essential for molecular modeling.\n",
        "**WHY**: SMILES strings are the standard input format for cheminformatics tools and models.  They are compact, efficient, and encode the molecular structure needed for advancded analyses.\n",
        "\n",
        "**activities**:\n",
        "Provides bioactivity metrics(e.g., IC50, Ki), which are critical for model labels.\n",
        "**WHY**: Bioactivity metrics from the labels for supervised learning models, helping predict the effectiveness or potency of molecules.\n",
        "\n",
        "**target_dictionary**:\n",
        "Contains target-level details, such as target type and associated proteins.\n",
        "**WHY**: Understanding the biological context of targets allows for more interpretable and biologically relevant predictions.\n",
        "\n",
        "**molecule_hierarchy**:\n",
        "Provides parent-child relationships between molecules (e.g., salts, hydrates, or parents).\n",
        "**WHY**: These relationships are useful for grouping related molecules and ensuring consistent labeling in models.\n",
        "\n",
        "**compound_properties**:\n",
        "Includes physiochemical attributes of molecules (e.g., molecular weight, logP, PSA).\n",
        "**WHY**: These descriptors enhance molecular feature sets and are commonly used in cheminformatics for predicting bioactivity or drug-likeness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Looking at the output from the selected tables, let's pick our key features for this phase of the project.\n",
        "\n",
        "**compound_structures**:  molregno, canonical_smiles, standard_inchi_key\n",
        "\n",
        "**activities**:  molregno, target_id, standard_value, standard_type\n",
        "\n",
        "**target_dictionary**:  target_id, pref_name, target_type\n",
        "\n",
        "**molecule_hierarchy**:  molregno, parent_molregno\n",
        "\n",
        "**compound_properties**:  molregno, full_mwt, alogp, psa, hba, hbd, rtb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 4:  Joining and Data Cleaning\n",
        "\n",
        "This section focuses on joining data from the prioritized tables into a unified dataset, followed by cleaning to ensure it is ready for the next step(s) and is ready for ingestion into Azure SQL.  We will break down the joins into small steps and review our progress to make sure we are progressing forward.  We will utilize chunking for batch processing as well as saving to csv in between each query in case errors happen due to the longer querying times so we can pick back up where we left off as a failsafe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 1:  Join compound_structures and compound_properties\n",
        "\n",
        "These tables are joined using molregno to combine molecular identifiers with physiochemical properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 5: Connection to Azure MySQL Database to Upload Data and Data Review\n",
        "\n",
        "We finally have the data we need at this time to send to our Azure MySQL database.  We will now connect to said database and upload the acquired dataset so we can use it at will during future phases of our project.  Before doing so however we will go through it quickly and review our features and see if there are any we can prune due to being redundant to our cause.  We will look to do this before making our submission to the server and moving on to the EDA portion of the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
