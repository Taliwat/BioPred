{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **ChEMBL Data Import and Preparation**\n",
        "\n",
        "## Objectives:\n",
        "\n",
        "The first notebook focuses on setting up the data pipeline by:\n",
        "\n",
        "1.  Reading the ChEMBL dataset directly from the URL.\n",
        "\n",
        "2.  Performing initial data cleaning and transformation.\n",
        "\n",
        "3.  Conducting exploratory analysis to identify critical tables, columns, and relationships.\n",
        "\n",
        "4.  Uploading the cleaned data to an Azure SQL Database.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 1: Import Libraries and Establish Project Root for Directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### First let's set our directory to the root of the project.  We will reuse this as well as it is a great way to keep yourself out of trouble with your directory issues, by setting your abspath to the root.  We can then in future notebooks refer back to the project_root when needed.  While we are doing this we will also create a path and variable to our data folder, since we will be referring back to it often."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root set to: /mnt/batch/tasks/shared/LS_root/mounts/clusters/kalpha18651/code/Users/kalpha1865/BioPred\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define project root\n",
        "project_root = \"/home/azureuser/cloudfiles/code/Users/kalpha1865/BioPred\"\n",
        "\n",
        "# Validate the directory\n",
        "if not os.path.exists(project_root):\n",
        "    raise FileNotFoundError(f\"Project root not found: {project_root}\")\n",
        "\n",
        "# Change working directory to project root if not already\n",
        "if os.getcwd() != project_root:\n",
        "    os.chdir(project_root)\n",
        "\n",
        "print(f\"Project root set to: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Now we can import the rest of our libraries, as well as establish a reference point to our Config file for our database credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1735953271559
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import requests\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "from sqlalchemy import create_engine\n",
        "import sqlite3\n",
        "import pyodbc\n",
        "import mysql.connector\n",
        "\n",
        "# Referencing the config file for Azure MySQL Database credentials.\n",
        "config_dir = os.path.join(project_root, \"Config\")\n",
        "sys.path.append(config_dir)\n",
        "\n",
        "from config import MYSQL_CONFIG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 2: Read and Extract Data from URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Now we will bring in our ChEMBL data from the site url.  We will send it to our data extracted subfolder for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "ConnectionError",
          "evalue": "HTTPSConnectionPool(host='ftp.ebi.ac.uk', port=443): Max retries exceeded with url: /pub/databases/chembl/ChEMBLdb/latest/chembl_35_sqlite.tar.gz (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4741671ff0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/urllib3/connectionpool.py:404\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/urllib3/connectionpool.py:1060\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1060\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/urllib3/connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7f4741671ff0>: Failed to establish a new connection: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/urllib3/connectionpool.py:801\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    799\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 801\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/urllib3/util/retry.py:594\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    596\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='ftp.ebi.ac.uk', port=443): Max retries exceeded with url: /pub/databases/chembl/ChEMBLdb/latest/chembl_35_sqlite.tar.gz (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4741671ff0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(extract_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Download the file from the url\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mopen(fileobj \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mraw, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr|gz\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[1;32m     10\u001b[0m     tar\u001b[38;5;241m.\u001b[39mextractall(path \u001b[38;5;241m=\u001b[39m extract_path)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='ftp.ebi.ac.uk', port=443): Max retries exceeded with url: /pub/databases/chembl/ChEMBLdb/latest/chembl_35_sqlite.tar.gz (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4741671ff0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
          ]
        }
      ],
      "source": [
        "data_url = \"https://ftp.ebi.ac.uk/pub/databases/chembl/ChEMBLdb/latest/chembl_35_sqlite.tar.gz\"\n",
        "extract_path = \"./data/extracted\"\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "#Download the file from the url\n",
        "response = requests.get(data_url, stream=True)\n",
        "\n",
        "with tarfile.open(fileobj = response.raw, mode = \"r|gz\") as tar:\n",
        "    tar.extractall(path = extract_path)\n",
        "\n",
        "print(f\"Files extracted to: {extract_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Let's take a preliminary look at the tables and see what is contained n the ChEMBL database.  We will use the provided schema to reference what information is available in the dataset and some of the existing relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://ftp.ebi.ac.uk/pub/databases/chembl/ChEMBLdb/latest/chembl_35_schema.png\" alt = \"ChEMBL Schema\" width = 2000>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 3: Data Exploration and Filtering\n",
        "\n",
        "SQLite is used to inspect the database schema and query relationships for efficient preparation of data for downstream processes in PySpark.  To avoid unnecessary overhead and ensure efficient data handling, the molfile column from the compound_structures table will be excluded from modeling workflows.  Instead, canonical_smiles will serve as the primary representation for molecular structures, as it is compact and fully compatible with RDKit and GNN workflows.\n",
        "\n",
        "Here are the tables we will be primarily interested in storing at this phase of the project:\n",
        "\n",
        "**compound_structures**:\n",
        "Contains molecule identifiers (SMILES, InChI) essential for molecular modeling.\n",
        "**WHY**: SMILES strings are the standard input format for cheminformatics tools and models.  They are compact, efficient, and encode the molecular structure needed for advancded analyses.\n",
        "\n",
        "**activities**:\n",
        "Provides bioactivity metrics(e.g., IC50, Ki), which are critical for model labels.\n",
        "**WHY**: Bioactivity metrics from the labels for supervised learning models, helping predict the effectiveness or potency of molecules.\n",
        "\n",
        "**target_dictionary**:\n",
        "Contains target-level details, such as target type and associated proteins.\n",
        "**WHY**: Understanding the biological context of targets allows for more interpretable and biologically relevant predictions.\n",
        "\n",
        "**molecule_hierarchy**:\n",
        "Provides parent-child relationships between molecules (e.g., salts, hydrates, or parents).\n",
        "**WHY**: These relationships are useful for grouping related molecules and ensuring consistent labeling in models.\n",
        "\n",
        "**compound_properties**:\n",
        "Includes physiochemical attributes of molecules (e.g., molecular weight, logP, PSA).\n",
        "**WHY**: These descriptors enhance molecular feature sets and are commonly used in cheminformatics for predicting bioactivity or drug-likeness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Let's load the SQLite database and perform some initial exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1735954701900
        }
      },
      "outputs": [],
      "source": [
        "# Make the initial connection to the SQLite db with our path to the ChEMBL data.\n",
        "db_path = \"/home/azureuser/cloudfiles/code/Users/kalpha1865/BioPred/data/extracted/chembl_35.db\"\n",
        "conn = sqlite3.connect(db_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema for compound_structures: [(0, 'molregno', 'BIGINT', 1, None, 1), (1, 'molfile', 'TEXT', 0, None, 0), (2, 'standard_inchi', 'VARCHAR(4000)', 0, None, 0), (3, 'standard_inchi_key', 'VARCHAR(27)', 1, None, 0), (4, 'canonical_smiles', 'VARCHAR(4000)', 0, None, 0)]\n",
            "Data from compound_structures:    molregno                                            molfile  \\\n",
            "0         1  \\n     RDKit          2D\\n\\n 24 26  0  0  0  0...   \n",
            "1         2  \\n     RDKit          2D\\n\\n 25 27  0  0  0  0...   \n",
            "2         3  \\n     RDKit          2D\\n\\n 25 27  0  0  0  0...   \n",
            "3         4  \\n     RDKit          2D\\n\\n 23 25  0  0  0  0...   \n",
            "4         5  \\n     RDKit          2D\\n\\n 24 26  0  0  0  0...   \n",
            "\n",
            "                                      standard_inchi  \\\n",
            "0  InChI=1S/C17H12ClN3O3/c1-10-8-11(21-17(24)20-1...   \n",
            "1  InChI=1S/C18H12N4O3/c1-11-8-14(22-18(25)21-16(...   \n",
            "2  InChI=1S/C18H16ClN3O3/c1-10-7-14(22-18(25)21-1...   \n",
            "3  InChI=1S/C17H13N3O3/c1-11-2-4-12(5-3-11)16(22)...   \n",
            "4  InChI=1S/C17H12ClN3O3/c1-10-8-13(21-17(24)20-1...   \n",
            "\n",
            "            standard_inchi_key  \\\n",
            "0  OWRSAHYFSSNENM-UHFFFAOYSA-N   \n",
            "1  ZJYUMURGSZQFMH-UHFFFAOYSA-N   \n",
            "2  YOMWDCALSDWFSV-UHFFFAOYSA-N   \n",
            "3  PSOPUAQFGCRDIP-UHFFFAOYSA-N   \n",
            "4  KEZNSCMBVRNOHO-UHFFFAOYSA-N   \n",
            "\n",
            "                                   canonical_smiles  \n",
            "0      Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccccc1Cl  \n",
            "1   Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(C#N)cc1  \n",
            "2  Cc1cc(-n2ncc(=O)[nH]c2=O)cc(C)c1C(O)c1ccc(Cl)cc1  \n",
            "3      Cc1ccc(C(=O)c2ccc(-n3ncc(=O)[nH]c3=O)cc2)cc1  \n",
            "4    Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(Cl)cc1  \n",
            "Schema for activities: [(0, 'activity_id', 'BIGINT', 1, None, 1), (1, 'assay_id', 'BIGINT', 1, None, 0), (2, 'doc_id', 'BIGINT', 0, None, 0), (3, 'record_id', 'BIGINT', 1, None, 0), (4, 'molregno', 'BIGINT', 0, None, 0), (5, 'standard_relation', 'VARCHAR(50)', 0, None, 0), (6, 'standard_value', 'NUMERIC', 0, None, 0), (7, 'standard_units', 'VARCHAR(100)', 0, None, 0), (8, 'standard_flag', 'SMALLINT', 0, None, 0), (9, 'standard_type', 'VARCHAR(250)', 0, None, 0), (10, 'activity_comment', 'VARCHAR(4000)', 0, None, 0), (11, 'data_validity_comment', 'VARCHAR(30)', 0, None, 0), (12, 'potential_duplicate', 'SMALLINT', 0, None, 0), (13, 'pchembl_value', 'NUMERIC(4, 2)', 0, None, 0), (14, 'bao_endpoint', 'VARCHAR(11)', 0, None, 0), (15, 'uo_units', 'VARCHAR(10)', 0, None, 0), (16, 'qudt_units', 'VARCHAR(70)', 0, None, 0), (17, 'toid', 'INTEGER', 0, None, 0), (18, 'upper_value', 'NUMERIC', 0, None, 0), (19, 'standard_upper_value', 'NUMERIC', 0, None, 0), (20, 'src_id', 'INTEGER', 0, None, 0), (21, 'type', 'VARCHAR(250)', 1, None, 0), (22, 'relation', 'VARCHAR(50)', 0, None, 0), (23, 'value', 'NUMERIC', 0, None, 0), (24, 'units', 'VARCHAR(100)', 0, None, 0), (25, 'text_value', 'VARCHAR(1000)', 0, None, 0), (26, 'standard_text_value', 'VARCHAR(1000)', 0, None, 0), (27, 'action_type', 'VARCHAR(50)', 0, None, 0)]\n",
            "Data from activities:    activity_id  assay_id  doc_id  record_id  molregno standard_relation  \\\n",
            "0        31863     54505    6424     206172    180094                 >   \n",
            "1        31864     83907    6432     208970    182268                 =   \n",
            "2        31865     88152    6432     208970    182268                 >   \n",
            "3        31866     83907    6432     208987    182855                 =   \n",
            "4        31867     88153    6432     208987    182855              None   \n",
            "\n",
            "   standard_value standard_units  standard_flag standard_type  ...  \\\n",
            "0        100000.0             nM              1          IC50  ...   \n",
            "1          2500.0             nM              1          IC50  ...   \n",
            "2         50000.0             nM              1          IC50  ...   \n",
            "3          9000.0             nM              1          IC50  ...   \n",
            "4             NaN             nM              0          IC50  ...   \n",
            "\n",
            "  upper_value standard_upper_value  src_id  type relation  value units  \\\n",
            "0        None                 None       1  IC50        >  100.0    uM   \n",
            "1        None                 None       1  IC50        =    2.5    uM   \n",
            "2        None                 None       1  IC50        >   50.0    uM   \n",
            "3        None                 None       1  IC50        =    9.0    uM   \n",
            "4        None                 None       1  IC50     None    NaN    uM   \n",
            "\n",
            "  text_value standard_text_value action_type  \n",
            "0       None                None        None  \n",
            "1       None                None        None  \n",
            "2       None                None        None  \n",
            "3       None                None        None  \n",
            "4       None                None        None  \n",
            "\n",
            "[5 rows x 28 columns]\n",
            "Schema for target_dictionary: [(0, 'tid', 'BIGINT', 1, None, 1), (1, 'target_type', 'VARCHAR(30)', 0, None, 0), (2, 'pref_name', 'VARCHAR(200)', 1, None, 0), (3, 'tax_id', 'BIGINT', 0, None, 0), (4, 'organism', 'VARCHAR(150)', 0, None, 0), (5, 'chembl_id', 'VARCHAR(20)', 1, None, 0), (6, 'species_group_flag', 'SMALLINT', 1, None, 0)]\n",
            "Data from target_dictionary:    tid     target_type                                          pref_name  \\\n",
            "0    1  SINGLE PROTEIN                               Maltase-glucoamylase   \n",
            "1    2  SINGLE PROTEIN                            Sulfonylurea receptor 2   \n",
            "2    3  SINGLE PROTEIN                               Phosphodiesterase 5A   \n",
            "3    4  SINGLE PROTEIN  Voltage-gated T-type calcium channel alpha-1H ...   \n",
            "4    5  SINGLE PROTEIN     Nicotinic acetylcholine receptor alpha subunit   \n",
            "\n",
            "   tax_id      organism   chembl_id  species_group_flag  \n",
            "0    9606  Homo sapiens  CHEMBL2074                   0  \n",
            "1    9606  Homo sapiens  CHEMBL1971                   0  \n",
            "2    9606  Homo sapiens  CHEMBL1827                   0  \n",
            "3    9606  Homo sapiens  CHEMBL1859                   0  \n",
            "4    6253  Ascaris suum  CHEMBL1884                   0  \n",
            "Schema for molecule_hierarchy: [(0, 'molregno', 'BIGINT', 1, None, 1), (1, 'parent_molregno', 'BIGINT', 0, None, 0), (2, 'active_molregno', 'BIGINT', 0, None, 0)]\n",
            "Data from molecule_hierarchy:    molregno  parent_molregno  active_molregno\n",
            "0         1                1                1\n",
            "1         2                2                2\n",
            "2         3                3                3\n",
            "3         4                4                4\n",
            "4         5                5                5\n",
            "Schema for compound_properties: [(0, 'molregno', 'BIGINT', 1, None, 1), (1, 'mw_freebase', 'NUMERIC(9, 2)', 0, None, 0), (2, 'alogp', 'NUMERIC(9, 2)', 0, None, 0), (3, 'hba', 'INTEGER', 0, None, 0), (4, 'hbd', 'INTEGER', 0, None, 0), (5, 'psa', 'NUMERIC(9, 2)', 0, None, 0), (6, 'rtb', 'INTEGER', 0, None, 0), (7, 'ro3_pass', 'VARCHAR(3)', 0, None, 0), (8, 'num_ro5_violations', 'SMALLINT', 0, None, 0), (9, 'cx_most_apka', 'NUMERIC(9, 2)', 0, None, 0), (10, 'cx_most_bpka', 'NUMERIC(9, 2)', 0, None, 0), (11, 'cx_logp', 'NUMERIC(9, 2)', 0, None, 0), (12, 'cx_logd', 'NUMERIC(9, 2)', 0, None, 0), (13, 'molecular_species', 'VARCHAR(50)', 0, None, 0), (14, 'full_mwt', 'NUMERIC(9, 2)', 0, None, 0), (15, 'aromatic_rings', 'INTEGER', 0, None, 0), (16, 'heavy_atoms', 'INTEGER', 0, None, 0), (17, 'qed_weighted', 'NUMERIC(3, 2)', 0, None, 0), (18, 'mw_monoisotopic', 'NUMERIC(11, 4)', 0, None, 0), (19, 'full_molformula', 'VARCHAR(100)', 0, None, 0), (20, 'hba_lipinski', 'INTEGER', 0, None, 0), (21, 'hbd_lipinski', 'INTEGER', 0, None, 0), (22, 'num_lipinski_ro5_violations', 'SMALLINT', 0, None, 0), (23, 'np_likeness_score', 'NUMERIC(3, 2)', 0, None, 0)]\n",
            "Data from compound_properties:    molregno  mw_freebase  alogp  hba  hbd     psa  rtb ro3_pass  \\\n",
            "0         1       341.75   2.11    5    1   84.82    3        N   \n",
            "1         2       332.32   1.33    6    1  108.61    3        N   \n",
            "2         3       357.80   2.27    5    2   87.98    3        N   \n",
            "3         4       307.31   1.46    5    1   84.82    3        N   \n",
            "4         5       341.75   2.11    5    1   84.82    3        N   \n",
            "\n",
            "   num_ro5_violations  cx_most_apka  ... full_mwt  aromatic_rings  \\\n",
            "0                   0          6.48  ...   341.75               3   \n",
            "1                   0          6.33  ...   332.32               3   \n",
            "2                   0          6.33  ...   357.80               3   \n",
            "3                   0          6.33  ...   307.31               3   \n",
            "4                   0          6.33  ...   341.75               3   \n",
            "\n",
            "   heavy_atoms qed_weighted  mw_monoisotopic  full_molformula  hba_lipinski  \\\n",
            "0           24         0.74         341.0567     C17H12ClN3O3             6   \n",
            "1           25         0.73         332.0909       C18H12N4O3             7   \n",
            "2           25         0.75         357.0880     C18H16ClN3O3             6   \n",
            "3           23         0.74         307.0957       C17H13N3O3             6   \n",
            "4           24         0.74         341.0567     C17H12ClN3O3             6   \n",
            "\n",
            "   hbd_lipinski  num_lipinski_ro5_violations np_likeness_score  \n",
            "0             1                            0             -1.56  \n",
            "1             1                            0             -1.59  \n",
            "2             2                            0             -0.82  \n",
            "3             1                            0             -1.10  \n",
            "4             1                            0             -1.49  \n",
            "\n",
            "[5 rows x 24 columns]\n"
          ]
        }
      ],
      "source": [
        "# Function to inspect the schema and preview data for an individual table\n",
        "def inspect_table(table_name, limit = 5):\n",
        "    schema_query = f\"PRAGMA table_info({table_name});\"\n",
        "    schema = conn.execute(schema_query).fetchall()\n",
        "    print(f\"Schema for {table_name}:\", schema)\n",
        "    \n",
        "    data_query = f\"SELECT * FROM {table_name} LIMIT {limit};\"\n",
        "    data = pd.read_sql_query(data_query, conn)\n",
        "    print(f\"Data from {table_name}:\", data.head())\n",
        "\n",
        "# Call the function for our specified tables above.\n",
        "inspect_table(\"compound_structures\")\n",
        "inspect_table(\"activities\")\n",
        "inspect_table(\"target_dictionary\")\n",
        "inspect_table(\"molecule_hierarchy\")\n",
        "inspect_table(\"compound_properties\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Looking at the output from the selected tables, let's pick our key features for this phase of the project.\n",
        "\n",
        "**compound_structures**:  molregno, canonical_smiles, standard_inchi_key\n",
        "\n",
        "**activities**:  molregno, target_id, standard_value, standard_type\n",
        "\n",
        "**target_dictionary**:  target_id, pref_name, target_type\n",
        "\n",
        "**molecule_hierarchy**:  molregno, parent_molregno\n",
        "\n",
        "**compound_properties**:  molregno, full_mwt, alogp, psa, hba, hbd, rtb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 4:  Joining and Data Cleaning\n",
        "\n",
        "This section focuses on joining data from the prioritized tables into a unified dataset, followed by cleaning to ensure it is ready for the next step(s) and is ready for ingestion into Azure SQL.  We will break down the joins into small steps and review our progress to make sure we are progressing forward.  We will utilize chunking for batch processing as well as saving to csv in between each query in case errors happen due to the longer querying times so we can pick back up where we left off as a failsafe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 1:  Join compound_structures and compound_properties\n",
        "\n",
        "These tables are joined using molregno to combine molecular identifiers with physiochemical properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1735953200412
        }
      },
      "outputs": [],
      "source": [
        "# using chunking to avoid kernel crashes and easier batch processing.\n",
        "# Very small chunk size due to the large dataset and multiple joins, the executions will take more time as a result.\n",
        "chunk_size = 50000\n",
        "data_chunks = []\n",
        "offset = 0\n",
        "\n",
        "join_query_1 = \"\"\"\n",
        "SELECT cs.CANONICAL_SMILES, cs.STANDARD_INCHI_KEY,\n",
        "    MIN(cp.FULL_MWT) AS FULL_MWT, MIN(cp.ALOGP) AS ALOGP,\n",
        "    MIN(cp.PSA) AS PSA, MIN(cp.HBA) AS HBA, MIN(cp.HBD) AS HBD,\n",
        "    MIN(cp.AROMATIC_RINGS) AS AROMATIC_RINGS, MIN(cp.NP_LIKENESS_SCORE) AS NP_LIKENESS_SCORE,\n",
        "    MIN(cp.HEAVY_ATOMS) AS HEAVY_ATOMS, MIN(cp.RTB) AS RTB, cp.MOLECULAR_SPECIES\n",
        "FROM COMPOUND_STRUCTURES cs\n",
        "LEFT JOIN COMPOUND_PROPERTIES cp ON cs.MOLREGNO = cp.MOLREGNO\n",
        "WHERE cp.FULL_MWT BETWEEN 200 AND 500\n",
        "    AND cp.RTB <= 10\n",
        "    AND cp.ALOGP BETWEEN -1 AND 5\n",
        "    AND cp.PSA <= 140\n",
        "    AND cp.HBD <= 5\n",
        "    AND cp.HBA <= 10\n",
        "GROUP BY cs.CANONICAL_SMILES, cs.STANDARD_INCHI_KEY, cp.MOLECULAR_SPECIES\n",
        "LIMIT ? OFFSET ?;\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    chunk = pd.read_sql_query(join_query_1, conn, params=(chunk_size, offset))\n",
        "    if chunk.empty:\n",
        "        break\n",
        "    data_chunks.append(chunk)\n",
        "    offset += chunk_size\n",
        "\n",
        "data_query_1 = pd.concat(data_chunks, ignore_index=True)\n",
        "print(data_query_1.head())\n",
        "print(data_query_1.shape)\n",
        "\n",
        "data_query_1.to_csv(\"./data/processed/data_query_1.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data_query_1 columns: Index(['molregno', 'canonical_smiles', 'standard_inchi_key', 'full_mwt',\n",
            "       'alogp', 'psa', 'hba', 'hbd', 'aromatic_rings', 'np_likeness_score',\n",
            "       'heavy_atoms', 'rtb', 'molecular_species'],\n",
            "      dtype='object')\n",
            "Processed chunk with offset 100000, Rows fetched: 100000\n",
            "Processed chunk with offset 200000, Rows fetched: 100000\n",
            "Processed chunk with offset 300000, Rows fetched: 100000\n",
            "Processed chunk with offset 400000, Rows fetched: 100000\n",
            "Processed chunk with offset 500000, Rows fetched: 100000\n",
            "Processed chunk with offset 600000, Rows fetched: 100000\n",
            "Processed chunk with offset 700000, Rows fetched: 100000\n",
            "Processed chunk with offset 800000, Rows fetched: 100000\n",
            "Processed chunk with offset 900000, Rows fetched: 100000\n",
            "Processed chunk with offset 1000000, Rows fetched: 100000\n",
            "Processed chunk with offset 1100000, Rows fetched: 100000\n",
            "Processed chunk with offset 1200000, Rows fetched: 100000\n",
            "Processed chunk with offset 1300000, Rows fetched: 100000\n",
            "Processed chunk with offset 1400000, Rows fetched: 100000\n",
            "Processed chunk with offset 1500000, Rows fetched: 100000\n",
            "Processed chunk with offset 1600000, Rows fetched: 100000\n",
            "Processed chunk with offset 1700000, Rows fetched: 100000\n",
            "Processed chunk with offset 1800000, Rows fetched: 100000\n",
            "Processed chunk with offset 1900000, Rows fetched: 100000\n",
            "Processed chunk with offset 2000000, Rows fetched: 100000\n",
            "Processed chunk with offset 2100000, Rows fetched: 100000\n",
            "Processed chunk with offset 2200000, Rows fetched: 100000\n",
            "Processed chunk with offset 2300000, Rows fetched: 100000\n",
            "Processed chunk with offset 2400000, Rows fetched: 100000\n",
            "Processed chunk with offset 2500000, Rows fetched: 100000\n",
            "Processed chunk with offset 2600000, Rows fetched: 100000\n",
            "Processed chunk with offset 2700000, Rows fetched: 100000\n",
            "Processed chunk with offset 2800000, Rows fetched: 100000\n",
            "Processed chunk with offset 2900000, Rows fetched: 100000\n",
            "Processed chunk with offset 3000000, Rows fetched: 100000\n",
            "Processed chunk with offset 3100000, Rows fetched: 100000\n",
            "Processed chunk with offset 3200000, Rows fetched: 100000\n",
            "Processed chunk with offset 3300000, Rows fetched: 100000\n",
            "Processed chunk with offset 3400000, Rows fetched: 100000\n",
            "Processed chunk with offset 3500000, Rows fetched: 100000\n",
            "Processed chunk with offset 3600000, Rows fetched: 100000\n",
            "Processed chunk with offset 3700000, Rows fetched: 100000\n",
            "Processed chunk with offset 3800000, Rows fetched: 100000\n",
            "Processed chunk with offset 3900000, Rows fetched: 100000\n"
          ]
        }
      ],
      "source": [
        "# Read in the CSV file from query_1\n",
        "data_query_1 = pd.read_csv(\"./data/processed/queried_data_step_1.csv\")\n",
        "\n",
        "# Verify key columns\n",
        "print(\"data_query_1 columns:\", data_query_1.columns)\n",
        "\n",
        "chunk_size = 100000\n",
        "data_chunks = []\n",
        "offset = 0\n",
        "\n",
        "# Adjusted query to include canonical_smiles via a join\n",
        "join_query_2 = \"\"\"\n",
        "SELECT cs.CANONICAL_SMILES,\n",
        "    MIN(a.STANDARD_VALUE) AS MIN_STANDARD_VALUE,\n",
        "    MAX(a.STANDARD_VALUE) AS MAX_STANDARD_VALUE,\n",
        "    COUNT(a.DOC_ID) AS ACTIVITY_COUNT\n",
        "FROM ACTIVITIES a\n",
        "LEFT JOIN COMPOUND_STRUCTURES cs ON a.MOLREGNO = cs.MOLREGNO\n",
        "WHERE a.standard_value <= 1000\n",
        "GROUP BY CANONICAL_SMILES\n",
        "LIMIT ? OFFSET ?;\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    # Fetch data in chunks\n",
        "    chunk = pd.read_sql_query(join_query_2, conn, params=(chunk_size, offset))\n",
        "    if chunk.empty:\n",
        "        break\n",
        "\n",
        "    # Merge the chunk with the CSV dataframe\n",
        "    merged_chunk = pd.merge(\n",
        "        data_query_1,\n",
        "        chunk,\n",
        "        on='CANONICAL_SMILES',\n",
        "        how='left'\n",
        "    )\n",
        "    data_chunks.append(merged_chunk)\n",
        "    offset += chunk_size\n",
        "\n",
        "    # Progress monitoring\n",
        "    print(f\"Processed chunk with offset {offset}, Rows fetched: {len(chunk)}\")\n",
        "\n",
        "# Combine all chunks\n",
        "data_query_2 = pd.concat(data_chunks, ignore_index=True)\n",
        "print(data_query_2.shape)\n",
        "print(data_query_2.head())\n",
        "\n",
        "# Save the final dataset\n",
        "data_query_2.to_csv(\"./data/processed/data_query_2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 2: Add Bioactivity Data from activities table.\n",
        "\n",
        "This step will introduce the metrics standard_value and standard_type.  We will use the built-in temp table variable s1 here to just add on to our previous work.  Also note here in this query we will filter our data based on the standard_value.  This will help in processing as well as help us determine how effective a molecule is at interacting with a biological target.  For this value, lower values indicate higher potency (i.e. the molecule is more effective at lower concentrations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9955476, 13)\n",
            "   molregno                                  canonical_smiles  \\\n",
            "0         1      Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccccc1Cl   \n",
            "1         1      Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccccc1Cl   \n",
            "2         2   Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(C#N)cc1   \n",
            "3         2   Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(C#N)cc1   \n",
            "4         3  Cc1cc(-n2ncc(=O)[nH]c2=O)cc(C)c1C(O)c1ccc(Cl)cc1   \n",
            "\n",
            "            standard_inchi_key  full_mwt  alogp     psa  hba  hbd  rtb  \\\n",
            "0  OWRSAHYFSSNENM-UHFFFAOYSA-N    341.75   2.11   84.82  5.0  1.0  3.0   \n",
            "1  OWRSAHYFSSNENM-UHFFFAOYSA-N    341.75   2.11   84.82  5.0  1.0  3.0   \n",
            "2  ZJYUMURGSZQFMH-UHFFFAOYSA-N    332.32   1.33  108.61  6.0  1.0  3.0   \n",
            "3  ZJYUMURGSZQFMH-UHFFFAOYSA-N    332.32   1.33  108.61  6.0  1.0  3.0   \n",
            "4  YOMWDCALSDWFSV-UHFFFAOYSA-N    357.80   2.27   87.98  5.0  2.0  3.0   \n",
            "\n",
            "   standard_value standard_type  doc_id  parent_molregno  \n",
            "0            7.08         pH1/2    7792                1  \n",
            "1           60.00           MEC    7792                1  \n",
            "2            7.08         pH1/2    7792                2  \n",
            "3          125.00           MEC    7792                2  \n",
            "4           30.00           MEC    7792                3  \n"
          ]
        }
      ],
      "source": [
        "# Step 3.1: Add hierarchy data from molecule_hierarchy.\n",
        "data_query_2 = pd.read_csv(\"./data/processed/data_query_2.csv\")\n",
        "\n",
        "# This time the logic will be a little different since we are just doing a quick join, can do the merge after declaring.\n",
        "molecule_hierarchy = pd.read_sql_query(\"SELECT MOLREGNO, PARENT_MOLREGNO FROM MOLECULE_HIERARCHY;\", conn)\n",
        "\n",
        "# Now set up the merge on our PK 'molregno'\n",
        "data_query_3 = pd.merge(\n",
        "    data_query_2,\n",
        "    molecule_hierarchy[['MOLREGNO', 'PARENT_MOLREGNO']],\n",
        "    on = 'MOLREGNO',\n",
        "    how = 'left'\n",
        ")\n",
        "\n",
        "print(data_query_3.shape)\n",
        "print(data_query_3.head())\n",
        "\n",
        "data_query_3.to_csv(\"./data/processed/data_query_3.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now add TID from assays table as a bridge for us to get to the target_dictionary table\n",
        "data_query_3 = pd.read_csv(\"./data/processed/data_query_3.csv\")\n",
        "\n",
        "chunk_size = 100000\n",
        "offset = 0\n",
        "data_chunks = []\n",
        "\n",
        "# Query to get data from the assays table, in doc_id and tid\n",
        "join_query_3 = \"\"\"\n",
        "SELECT DOC_ID, TID\n",
        "FROM ASSAYS\n",
        "LIMIT ? OFFSET ?;\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    chunk = pd.read_sql_query(join_query_3, conn, params=(chunk_size, offset))\n",
        "\n",
        "    if chunk.empty:\n",
        "        break\n",
        "    \n",
        "    data_chunks.append(chunk)\n",
        "    offset += chunk_size\n",
        "\n",
        "assays_data = pd.concat(data_chunks, ignore_index = True)\n",
        "\n",
        "data_query_4 = pd.merge(\n",
        "    data_query_3,\n",
        "    assays_data,\n",
        "    on = 'DOC_ID',\n",
        "    how = 'left'\n",
        ")\n",
        "\n",
        "print(data_query_4.head())\n",
        "print(data_query_4.shape)\n",
        "\n",
        "data_query_4.to_csv(\"./data/processed/data_query_4.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in our csv file once more\n",
        "data_query_4 = pd.read_csv(\"./data/processed/data_query_4.csv\")\n",
        "\n",
        "# No need for the doc_id column anymore, just drop now.\n",
        "data_query_4 = data_query_4.drop(columns = ['DOC_ID'])\n",
        "\n",
        "chunk_size = 100000\n",
        "offset = 0\n",
        "data_chunks = []\n",
        "\n",
        "join_query_4 = \"\"\"\n",
        "SELECT PREF_NAME, TARGET_NAME, TARGET_TYPE, TID\n",
        "FROM TARGET_DICTIONARY\n",
        "LIMIT ? OFFSET ?;\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    chunk = pd.read_sql_query(join_query_4, conn, params=(chunk_size, offset))\n",
        "    if chunk.empty:\n",
        "        break\n",
        "    \n",
        "    data_chunks.append(chunk)\n",
        "    offset += chunk_size\n",
        "    \n",
        "target_dictionary_data = pd.concat(data_chunks, ignore_index=True)\n",
        "\n",
        "data_query_5 = pd.merge(\n",
        "    data_query_4,\n",
        "    target_dictionary_data,\n",
        "    on = 'TID',\n",
        "    how = 'left'\n",
        ")\n",
        "\n",
        "print(data_query_5.head())\n",
        "print(data_query_5.shape)\n",
        "\n",
        "data_query_5.to_csv(\"./data/processed/eda_db.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Important!  Close the connection\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 5: Connection to Azure MySQL Database to Upload Data and Data Review\n",
        "\n",
        "We finally have the data we need at this time to send to our Azure MySQL database.  We will now connect to said database and upload the acquired dataset so we can use it at will during future phases of our project.  Before doing so however we will go through it quickly and review our features and see if there are any we can prune due to being redundant to our cause.  We will look to do this before making our submission to the server and moving on to the EDA portion of the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's read in our eda_db file and review the features.\n",
        "eda_db = pd.read_csv(\"./data/processed/eda_db.csv\")\n",
        "\n",
        "print(eda_db.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to the Azure MySQL Database:\n",
        "try:\n",
        "        conn = mysql.connector.connect(\n",
        "                hostname = MYSQL_CONFIG[\"hostname\"],\n",
        "                port = MYSQL_CONFIG[\"port\"],\n",
        "                username = MYSQL_CONFIG[\"username\"],\n",
        "                password = MYSQL_CONFIG[\"password\"],\n",
        "                database = MYSQL_CONFIG[\"database\"],\n",
        "                ssl_mode = MYSQL_CONFIG[\"ssl_mode\"]\n",
        "        )\n",
        "        print(\"Connected to the Azure MySQL Database successfully!\")\n",
        "except mysql.connector.Error as e:\n",
        "        print(f\"Error connecting to the database: {e}\")\n",
        "        exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    # Need to create a new table in the database,\n",
        "    # this will provide the foundational structure for our pending data upload.\n",
        "    table_name = \"chembl_data\"\n",
        "    create_table_query = f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
        "        {', '.join([f'{col} VARCHAR(255)' for col in eda_db.columns])}\n",
        "    );\n",
        "    \"\"\"\n",
        "    cursor.execute(create_table_query)\n",
        "    print(f\"Table '{table_name} created or verified successfully!\")\n",
        "    \n",
        "    # Now we will insert and upload our data to our table we just created.\n",
        "    insert_query = f\"\"\"\n",
        "    INSERT INTO {table_name} ({', '.join(eda_db.columns)})\n",
        "    VALUES ({', '.join(['%s' for _ in eda_db.columns])});\n",
        "    \"\"\"\n",
        "    \n",
        "    cursor.exectutemany(insert_query, eda_db.values.tolist())\n",
        "    conn.commit()\n",
        "    \n",
        "    print(f\"Data uploaded successfully to table '{table_name}'.\")\n",
        "except mysql.connector.Error as e:\n",
        "    print(f\"Error uploading data: {e}\")\n",
        "finally:\n",
        "    cursor.close()\n",
        "    conn.close()\n",
        "    print(\"Database connection closed.\")\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
