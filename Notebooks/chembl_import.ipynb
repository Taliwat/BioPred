{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **ChEMBL Data Import and Preparation**\n",
        "\n",
        "## Objectives:\n",
        "\n",
        "The first notebook focuses on setting up the data pipeline by:\n",
        "\n",
        "1.  Reading the ChEMBL dataset directly from the URL.\n",
        "\n",
        "2.  Performing initial data cleaning and transformation.\n",
        "\n",
        "3.  Conducting exploratory analysis to identify critical tables, columns, and relationships.\n",
        "\n",
        "4.  Uploading the cleaned data to an Azure SQL Database.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 1: Import Libraries and Establish Project Root for Directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### First let's set our directory to the root of the project.  We will reuse this as well as it is a great way to keep yourself out of trouble with your directory issues, by setting your abspath to the root.  We can then in future notebooks refer back to the project_root when needed.  While we are doing this we will also create a path and variable to our data folder, since we will be referring back to it often."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root set to: /mnt/batch/tasks/shared/LS_root/mounts/clusters/kalpha18651/code/Users/kalpha1865/BioPred\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define project root\n",
        "project_root = \"/home/azureuser/cloudfiles/code/Users/kalpha1865/BioPred\"\n",
        "\n",
        "# Validate the directory\n",
        "if not os.path.exists(project_root):\n",
        "    raise FileNotFoundError(f\"Project root not found: {project_root}\")\n",
        "\n",
        "# Change working directory to project root if not already\n",
        "if os.getcwd() != project_root:\n",
        "    os.chdir(project_root)\n",
        "\n",
        "print(f\"Project root set to: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Now we can import the rest of our libraries, as well as establish a reference point to our Config file for our database credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1735953271559
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import requests\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "from sqlalchemy import create_engine\n",
        "import sqlite3\n",
        "import pyodbc\n",
        "import mysql.connector\n",
        "\n",
        "# Referencing the config file for Azure MySQL Database credentials.\n",
        "config_dir = os.path.join(project_root, \"Config\")\n",
        "sys.path.append(config_dir)\n",
        "\n",
        "from config import MYSQL_CONFIG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 2: Read and Extract Data from URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Now we will bring in our ChEMBL data from the site url.  We will send it to our data extracted subfolder for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(data_url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mopen(fileobj \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mraw, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr|gz\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mextract_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles extracted to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/tarfile.py:2059\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   2057\u001b[0m         tarinfo\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0o700\u001b[39m\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# Do not set_attrs directories, as we will do that further down\u001b[39;00m\n\u001b[0;32m-> 2059\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[38;5;66;03m# Reverse sort directories.\u001b[39;00m\n\u001b[1;32m   2063\u001b[0m directories\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m a: a\u001b[38;5;241m.\u001b[39mname)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/tarfile.py:2100\u001b[0m, in \u001b[0;36mTarFile.extract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2097\u001b[0m     tarinfo\u001b[38;5;241m.\u001b[39m_link_target \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, tarinfo\u001b[38;5;241m.\u001b[39mlinkname)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrorlevel \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/tarfile.py:2173\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbg(\u001b[38;5;241m1\u001b[39m, tarinfo\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   2172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misreg():\n\u001b[0;32m-> 2173\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[1;32m   2175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedir(tarinfo, targetpath)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/tarfile.py:2222\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2220\u001b[0m     target\u001b[38;5;241m.\u001b[39mtruncate()\n\u001b[1;32m   2221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2222\u001b[0m     \u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mReadError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/tarfile.py:248\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    246\u001b[0m blocks, remainder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(length, bufsize)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(blocks):\n\u001b[0;32m--> 248\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf) \u001b[38;5;241m<\u001b[39m bufsize:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/tarfile.py:522\u001b[0m, in \u001b[0;36m_Stream.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the next size number of bytes from the stream.\"\"\"\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buf\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/tarfile.py:540\u001b[0m, in \u001b[0;36m_Stream._read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buf:\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "data_url = \"https://ftp.ebi.ac.uk/pub/databases/chembl/ChEMBLdb/latest/chembl_35_sqlite.tar.gz\"\n",
        "extract_path = \"./data/extracted\"\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "#Download the file from the url\n",
        "response = requests.get(data_url, stream=True)\n",
        "\n",
        "with tarfile.open(fileobj = response.raw, mode = \"r|gz\") as tar:\n",
        "    tar.extractall(path = extract_path)\n",
        "\n",
        "print(f\"Files extracted to: {extract_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Let's take a preliminary look at the tables and see what is contained n the ChEMBL database.  We will use the provided schema to reference what information is available in the dataset and some of the existing relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://ftp.ebi.ac.uk/pub/databases/chembl/ChEMBLdb/latest/chembl_35_schema.png\" alt = \"ChEMBL Schema\" width = 2000>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 3: Data Exploration and Filtering\n",
        "\n",
        "SQLite is used to inspect the database schema and query relationships for efficient preparation of data for downstream processes in PySpark.  To avoid unnecessary overhead and ensure efficient data handling, the molfile column from the compound_structures table will be excluded from modeling workflows.  Instead, canonical_smiles will serve as the primary representation for molecular structures, as it is compact and fully compatible with RDKit and GNN workflows.\n",
        "\n",
        "Here are the tables we will be primarily interested in storing at this phase of the project:\n",
        "\n",
        "**compound_structures**:\n",
        "Contains molecule identifiers (SMILES, InChI) essential for molecular modeling.\n",
        "**WHY**: SMILES strings are the standard input format for cheminformatics tools and models.  They are compact, efficient, and encode the molecular structure needed for advancded analyses.\n",
        "\n",
        "**activities**:\n",
        "Provides bioactivity metrics(e.g., IC50, Ki), which are critical for model labels.\n",
        "**WHY**: Bioactivity metrics from the labels for supervised learning models, helping predict the effectiveness or potency of molecules.\n",
        "\n",
        "**target_dictionary**:\n",
        "Contains target-level details, such as target type and associated proteins.\n",
        "**WHY**: Understanding the biological context of targets allows for more interpretable and biologically relevant predictions.\n",
        "\n",
        "**molecule_hierarchy**:\n",
        "Provides parent-child relationships between molecules (e.g., salts, hydrates, or parents).\n",
        "**WHY**: These relationships are useful for grouping related molecules and ensuring consistent labeling in models.\n",
        "\n",
        "**compound_properties**:\n",
        "Includes physiochemical attributes of molecules (e.g., molecular weight, logP, PSA).\n",
        "**WHY**: These descriptors enhance molecular feature sets and are commonly used in cheminformatics for predicting bioactivity or drug-likeness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Let's load the SQLite database and perform some initial exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1735954701900
        }
      },
      "outputs": [],
      "source": [
        "# Make the initial connection to the SQLite db with our path to the ChEMBL data.\n",
        "db_path = \"/home/azureuser/cloudfiles/code/Users/kalpha1865/BioPred/data/extracted/chembl_35.db\"\n",
        "conn = sqlite3.connect(db_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema for compound_structures: [(0, 'molregno', 'BIGINT', 1, None, 1), (1, 'molfile', 'TEXT', 0, None, 0), (2, 'standard_inchi', 'VARCHAR(4000)', 0, None, 0), (3, 'standard_inchi_key', 'VARCHAR(27)', 1, None, 0), (4, 'canonical_smiles', 'VARCHAR(4000)', 0, None, 0)]\n",
            "Data from compound_structures:    molregno                                            molfile  \\\n",
            "0         1  \\n     RDKit          2D\\n\\n 24 26  0  0  0  0...   \n",
            "1         2  \\n     RDKit          2D\\n\\n 25 27  0  0  0  0...   \n",
            "2         3  \\n     RDKit          2D\\n\\n 25 27  0  0  0  0...   \n",
            "3         4  \\n     RDKit          2D\\n\\n 23 25  0  0  0  0...   \n",
            "4         5  \\n     RDKit          2D\\n\\n 24 26  0  0  0  0...   \n",
            "\n",
            "                                      standard_inchi  \\\n",
            "0  InChI=1S/C17H12ClN3O3/c1-10-8-11(21-17(24)20-1...   \n",
            "1  InChI=1S/C18H12N4O3/c1-11-8-14(22-18(25)21-16(...   \n",
            "2  InChI=1S/C18H16ClN3O3/c1-10-7-14(22-18(25)21-1...   \n",
            "3  InChI=1S/C17H13N3O3/c1-11-2-4-12(5-3-11)16(22)...   \n",
            "4  InChI=1S/C17H12ClN3O3/c1-10-8-13(21-17(24)20-1...   \n",
            "\n",
            "            standard_inchi_key  \\\n",
            "0  OWRSAHYFSSNENM-UHFFFAOYSA-N   \n",
            "1  ZJYUMURGSZQFMH-UHFFFAOYSA-N   \n",
            "2  YOMWDCALSDWFSV-UHFFFAOYSA-N   \n",
            "3  PSOPUAQFGCRDIP-UHFFFAOYSA-N   \n",
            "4  KEZNSCMBVRNOHO-UHFFFAOYSA-N   \n",
            "\n",
            "                                   canonical_smiles  \n",
            "0      Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccccc1Cl  \n",
            "1   Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(C#N)cc1  \n",
            "2  Cc1cc(-n2ncc(=O)[nH]c2=O)cc(C)c1C(O)c1ccc(Cl)cc1  \n",
            "3      Cc1ccc(C(=O)c2ccc(-n3ncc(=O)[nH]c3=O)cc2)cc1  \n",
            "4    Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(Cl)cc1  \n",
            "Schema for activities: [(0, 'activity_id', 'BIGINT', 1, None, 1), (1, 'assay_id', 'BIGINT', 1, None, 0), (2, 'doc_id', 'BIGINT', 0, None, 0), (3, 'record_id', 'BIGINT', 1, None, 0), (4, 'molregno', 'BIGINT', 0, None, 0), (5, 'standard_relation', 'VARCHAR(50)', 0, None, 0), (6, 'standard_value', 'NUMERIC', 0, None, 0), (7, 'standard_units', 'VARCHAR(100)', 0, None, 0), (8, 'standard_flag', 'SMALLINT', 0, None, 0), (9, 'standard_type', 'VARCHAR(250)', 0, None, 0), (10, 'activity_comment', 'VARCHAR(4000)', 0, None, 0), (11, 'data_validity_comment', 'VARCHAR(30)', 0, None, 0), (12, 'potential_duplicate', 'SMALLINT', 0, None, 0), (13, 'pchembl_value', 'NUMERIC(4, 2)', 0, None, 0), (14, 'bao_endpoint', 'VARCHAR(11)', 0, None, 0), (15, 'uo_units', 'VARCHAR(10)', 0, None, 0), (16, 'qudt_units', 'VARCHAR(70)', 0, None, 0), (17, 'toid', 'INTEGER', 0, None, 0), (18, 'upper_value', 'NUMERIC', 0, None, 0), (19, 'standard_upper_value', 'NUMERIC', 0, None, 0), (20, 'src_id', 'INTEGER', 0, None, 0), (21, 'type', 'VARCHAR(250)', 1, None, 0), (22, 'relation', 'VARCHAR(50)', 0, None, 0), (23, 'value', 'NUMERIC', 0, None, 0), (24, 'units', 'VARCHAR(100)', 0, None, 0), (25, 'text_value', 'VARCHAR(1000)', 0, None, 0), (26, 'standard_text_value', 'VARCHAR(1000)', 0, None, 0), (27, 'action_type', 'VARCHAR(50)', 0, None, 0)]\n",
            "Data from activities:    activity_id  assay_id  doc_id  record_id  molregno standard_relation  \\\n",
            "0        31863     54505    6424     206172    180094                 >   \n",
            "1        31864     83907    6432     208970    182268                 =   \n",
            "2        31865     88152    6432     208970    182268                 >   \n",
            "3        31866     83907    6432     208987    182855                 =   \n",
            "4        31867     88153    6432     208987    182855              None   \n",
            "\n",
            "   standard_value standard_units  standard_flag standard_type  ...  \\\n",
            "0        100000.0             nM              1          IC50  ...   \n",
            "1          2500.0             nM              1          IC50  ...   \n",
            "2         50000.0             nM              1          IC50  ...   \n",
            "3          9000.0             nM              1          IC50  ...   \n",
            "4             NaN             nM              0          IC50  ...   \n",
            "\n",
            "  upper_value standard_upper_value  src_id  type relation  value units  \\\n",
            "0        None                 None       1  IC50        >  100.0    uM   \n",
            "1        None                 None       1  IC50        =    2.5    uM   \n",
            "2        None                 None       1  IC50        >   50.0    uM   \n",
            "3        None                 None       1  IC50        =    9.0    uM   \n",
            "4        None                 None       1  IC50     None    NaN    uM   \n",
            "\n",
            "  text_value standard_text_value action_type  \n",
            "0       None                None        None  \n",
            "1       None                None        None  \n",
            "2       None                None        None  \n",
            "3       None                None        None  \n",
            "4       None                None        None  \n",
            "\n",
            "[5 rows x 28 columns]\n",
            "Schema for target_dictionary: [(0, 'tid', 'BIGINT', 1, None, 1), (1, 'target_type', 'VARCHAR(30)', 0, None, 0), (2, 'pref_name', 'VARCHAR(200)', 1, None, 0), (3, 'tax_id', 'BIGINT', 0, None, 0), (4, 'organism', 'VARCHAR(150)', 0, None, 0), (5, 'chembl_id', 'VARCHAR(20)', 1, None, 0), (6, 'species_group_flag', 'SMALLINT', 1, None, 0)]\n",
            "Data from target_dictionary:    tid     target_type                                          pref_name  \\\n",
            "0    1  SINGLE PROTEIN                               Maltase-glucoamylase   \n",
            "1    2  SINGLE PROTEIN                            Sulfonylurea receptor 2   \n",
            "2    3  SINGLE PROTEIN                               Phosphodiesterase 5A   \n",
            "3    4  SINGLE PROTEIN  Voltage-gated T-type calcium channel alpha-1H ...   \n",
            "4    5  SINGLE PROTEIN     Nicotinic acetylcholine receptor alpha subunit   \n",
            "\n",
            "   tax_id      organism   chembl_id  species_group_flag  \n",
            "0    9606  Homo sapiens  CHEMBL2074                   0  \n",
            "1    9606  Homo sapiens  CHEMBL1971                   0  \n",
            "2    9606  Homo sapiens  CHEMBL1827                   0  \n",
            "3    9606  Homo sapiens  CHEMBL1859                   0  \n",
            "4    6253  Ascaris suum  CHEMBL1884                   0  \n",
            "Schema for molecule_hierarchy: [(0, 'molregno', 'BIGINT', 1, None, 1), (1, 'parent_molregno', 'BIGINT', 0, None, 0), (2, 'active_molregno', 'BIGINT', 0, None, 0)]\n",
            "Data from molecule_hierarchy:    molregno  parent_molregno  active_molregno\n",
            "0         1                1                1\n",
            "1         2                2                2\n",
            "2         3                3                3\n",
            "3         4                4                4\n",
            "4         5                5                5\n",
            "Schema for compound_properties: [(0, 'molregno', 'BIGINT', 1, None, 1), (1, 'mw_freebase', 'NUMERIC(9, 2)', 0, None, 0), (2, 'alogp', 'NUMERIC(9, 2)', 0, None, 0), (3, 'hba', 'INTEGER', 0, None, 0), (4, 'hbd', 'INTEGER', 0, None, 0), (5, 'psa', 'NUMERIC(9, 2)', 0, None, 0), (6, 'rtb', 'INTEGER', 0, None, 0), (7, 'ro3_pass', 'VARCHAR(3)', 0, None, 0), (8, 'num_ro5_violations', 'SMALLINT', 0, None, 0), (9, 'cx_most_apka', 'NUMERIC(9, 2)', 0, None, 0), (10, 'cx_most_bpka', 'NUMERIC(9, 2)', 0, None, 0), (11, 'cx_logp', 'NUMERIC(9, 2)', 0, None, 0), (12, 'cx_logd', 'NUMERIC(9, 2)', 0, None, 0), (13, 'molecular_species', 'VARCHAR(50)', 0, None, 0), (14, 'full_mwt', 'NUMERIC(9, 2)', 0, None, 0), (15, 'aromatic_rings', 'INTEGER', 0, None, 0), (16, 'heavy_atoms', 'INTEGER', 0, None, 0), (17, 'qed_weighted', 'NUMERIC(3, 2)', 0, None, 0), (18, 'mw_monoisotopic', 'NUMERIC(11, 4)', 0, None, 0), (19, 'full_molformula', 'VARCHAR(100)', 0, None, 0), (20, 'hba_lipinski', 'INTEGER', 0, None, 0), (21, 'hbd_lipinski', 'INTEGER', 0, None, 0), (22, 'num_lipinski_ro5_violations', 'SMALLINT', 0, None, 0), (23, 'np_likeness_score', 'NUMERIC(3, 2)', 0, None, 0)]\n",
            "Data from compound_properties:    molregno  mw_freebase  alogp  hba  hbd     psa  rtb ro3_pass  \\\n",
            "0         1       341.75   2.11    5    1   84.82    3        N   \n",
            "1         2       332.32   1.33    6    1  108.61    3        N   \n",
            "2         3       357.80   2.27    5    2   87.98    3        N   \n",
            "3         4       307.31   1.46    5    1   84.82    3        N   \n",
            "4         5       341.75   2.11    5    1   84.82    3        N   \n",
            "\n",
            "   num_ro5_violations  cx_most_apka  ... full_mwt  aromatic_rings  \\\n",
            "0                   0          6.48  ...   341.75               3   \n",
            "1                   0          6.33  ...   332.32               3   \n",
            "2                   0          6.33  ...   357.80               3   \n",
            "3                   0          6.33  ...   307.31               3   \n",
            "4                   0          6.33  ...   341.75               3   \n",
            "\n",
            "   heavy_atoms qed_weighted  mw_monoisotopic  full_molformula  hba_lipinski  \\\n",
            "0           24         0.74         341.0567     C17H12ClN3O3             6   \n",
            "1           25         0.73         332.0909       C18H12N4O3             7   \n",
            "2           25         0.75         357.0880     C18H16ClN3O3             6   \n",
            "3           23         0.74         307.0957       C17H13N3O3             6   \n",
            "4           24         0.74         341.0567     C17H12ClN3O3             6   \n",
            "\n",
            "   hbd_lipinski  num_lipinski_ro5_violations np_likeness_score  \n",
            "0             1                            0             -1.56  \n",
            "1             1                            0             -1.59  \n",
            "2             2                            0             -0.82  \n",
            "3             1                            0             -1.10  \n",
            "4             1                            0             -1.49  \n",
            "\n",
            "[5 rows x 24 columns]\n"
          ]
        }
      ],
      "source": [
        "# Function to inspect the schema and preview data for an individual table\n",
        "def inspect_table(table_name, limit = 5):\n",
        "    schema_query = f\"PRAGMA table_info({table_name});\"\n",
        "    schema = conn.execute(schema_query).fetchall()\n",
        "    print(f\"Schema for {table_name}:\", schema)\n",
        "    \n",
        "    data_query = f\"SELECT * FROM {table_name} LIMIT {limit};\"\n",
        "    data = pd.read_sql_query(data_query, conn)\n",
        "    print(f\"Data from {table_name}:\", data.head())\n",
        "\n",
        "# Call the function for our specified tables above.\n",
        "inspect_table(\"compound_structures\")\n",
        "inspect_table(\"activities\")\n",
        "inspect_table(\"target_dictionary\")\n",
        "inspect_table(\"molecule_hierarchy\")\n",
        "inspect_table(\"compound_properties\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Looking at the output from the selected tables, let's pick our key features for this phase of the project.\n",
        "\n",
        "**compound_structures**:  molregno, canonical_smiles, standard_inchi_key\n",
        "\n",
        "**activities**:  molregno, target_id, standard_value, standard_type\n",
        "\n",
        "**target_dictionary**:  target_id, pref_name, target_type\n",
        "\n",
        "**molecule_hierarchy**:  molregno, parent_molregno\n",
        "\n",
        "**compound_properties**:  molregno, full_mwt, alogp, psa, hba, hbd, rtb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 4:  Joining and Data Cleaning\n",
        "\n",
        "This section focuses on joining data from the prioritized tables into a unified dataset, followed by cleaning to ensure it is ready for the next step(s) and is ready for ingestion into Azure SQL.  We will break down the joins into small steps and review our progress to make sure we are progressing forward.  We will utilize chunking for batch processing as well as saving to csv in between each query in case errors happen due to the longer querying times so we can pick back up where we left off as a failsafe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 1:  Join compound_structures and compound_properties\n",
        "\n",
        "These tables are joined using molregno to combine molecular identifiers with physiochemical properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1735953200412
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   molregno                                   canonical_smiles  \\\n",
            "0     27502                 CC(C)(C)OC(=O)NCC/N=C(\\NN)NCC(=O)O   \n",
            "1     32210                 CC(=O)OCCn1cnc2c1c(=O)n(C)c(=O)n2C   \n",
            "2     47802  C[C@H]1[C@H](NC(=O)Cc2csc(N)n2)C(=O)N1OCC(=O)[...   \n",
            "3     73646                    O=c1[nH]c(=O)n(C2COC(CO)O2)cc1I   \n",
            "4     73647                        Cc1cn(C2COC(CO)O2)c(=O)nc1N   \n",
            "\n",
            "            standard_inchi_key  full_mwt  alogp     psa  hba  hbd  \\\n",
            "0  OYJKAGYHSGFSGQ-UHFFFAOYSA-N    275.31   -1.0  138.07    5    5   \n",
            "1  CQKZJBMFMGGBHJ-UHFFFAOYSA-N    266.26   -1.0   88.12    8    0   \n",
            "2  QRVMRNMOBASNLF-WFZUHFMFSA-M    352.41   -1.0  134.85    7    3   \n",
            "3  GFRQBEMRUYIKAQ-UHFFFAOYSA-N    340.07   -1.0   93.55    6    2   \n",
            "4  MJGOECXTQVPSJN-UHFFFAOYSA-N    227.22   -1.0   99.60    7    2   \n",
            "\n",
            "   aromatic_rings  np_likeness_score  heavy_atoms  rtb molecular_species  \n",
            "0               0              -0.44           19    5        ZWITTERION  \n",
            "1               2              -0.99           19    3           NEUTRAL  \n",
            "2               1              -1.01           21    6              ACID  \n",
            "3               1               0.41           16    2           NEUTRAL  \n",
            "4               1               0.61           16    2           NEUTRAL  \n",
            "(1601980, 13)\n"
          ]
        }
      ],
      "source": [
        "# using chunking to avoid kernel crashes and easier batch processing.\n",
        "# Very small chunk size due to the large dataset and multiple joins, the executions will take more time as a result.\n",
        "chunk_size = 50000\n",
        "data_chunks = []\n",
        "offset = 0\n",
        "\n",
        "join_query_1 = \"\"\"\n",
        "SELECT cs.MOLREGNO, cs.CANONICAL_SMILES, cs.STANDARD_INCHI_KEY, cp.FULL_MWT,\n",
        "cp.ALOGP, cp.PSA, cp.HBA, cp.HBD, cp.AROMATIC_RINGS, cp.NP_LIKENESS_SCORE,\n",
        "cp.HEAVY_ATOMS, cp.RTB, cp.MOLECULAR_SPECIES\n",
        "FROM COMPOUND_STRUCTURES cs\n",
        "LEFT JOIN COMPOUND_PROPERTIES cp ON cs.MOLREGNO = cp.MOLREGNO\n",
        "WHERE cp.FULL_MWT BETWEEN 200 AND 500\n",
        "    AND cp.RTB <= 10\n",
        "    AND cp.ALOGP BETWEEN -1 AND 5\n",
        "    AND cp.PSA <= 140\n",
        "    AND cp.HBD <= 5\n",
        "    AND cp.HBA <= 10\n",
        "LIMIT ? OFFSET ?;\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    chunk = pd.read_sql_query(join_query_1, conn, params=(chunk_size, offset))\n",
        "    if chunk.empty:\n",
        "        break\n",
        "    data_chunks.append(chunk)\n",
        "    offset += chunk_size\n",
        "\n",
        "data_query_1 = pd.concat(data_chunks, ignore_index=True)\n",
        "print(data_query_1.head())\n",
        "print(data_query_1.shape)\n",
        "\n",
        "data_query_1.to_csv(\"./data/processed/queried_data_step_1.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data_query_1 columns: Index(['molregno', 'canonical_smiles', 'standard_inchi_key', 'full_mwt',\n",
            "       'alogp', 'psa', 'hba', 'hbd', 'aromatic_rings', 'np_likeness_score',\n",
            "       'heavy_atoms', 'rtb', 'molecular_species'],\n",
            "      dtype='object')\n",
            "Processed chunk with offset 100000, Rows fetched: 100000\n",
            "Processed chunk with offset 200000, Rows fetched: 100000\n",
            "Processed chunk with offset 300000, Rows fetched: 100000\n",
            "Processed chunk with offset 400000, Rows fetched: 100000\n",
            "Processed chunk with offset 500000, Rows fetched: 100000\n",
            "Processed chunk with offset 600000, Rows fetched: 100000\n",
            "Processed chunk with offset 700000, Rows fetched: 100000\n",
            "Processed chunk with offset 800000, Rows fetched: 100000\n",
            "Processed chunk with offset 900000, Rows fetched: 100000\n",
            "Processed chunk with offset 1000000, Rows fetched: 100000\n",
            "Processed chunk with offset 1100000, Rows fetched: 100000\n",
            "Processed chunk with offset 1200000, Rows fetched: 100000\n",
            "Processed chunk with offset 1300000, Rows fetched: 100000\n",
            "Processed chunk with offset 1400000, Rows fetched: 100000\n",
            "Processed chunk with offset 1500000, Rows fetched: 100000\n",
            "Processed chunk with offset 1600000, Rows fetched: 100000\n",
            "Processed chunk with offset 1700000, Rows fetched: 100000\n",
            "Processed chunk with offset 1800000, Rows fetched: 100000\n",
            "Processed chunk with offset 1900000, Rows fetched: 100000\n",
            "Processed chunk with offset 2000000, Rows fetched: 100000\n",
            "Processed chunk with offset 2100000, Rows fetched: 100000\n",
            "Processed chunk with offset 2200000, Rows fetched: 100000\n",
            "Processed chunk with offset 2300000, Rows fetched: 100000\n",
            "Processed chunk with offset 2400000, Rows fetched: 100000\n",
            "Processed chunk with offset 2500000, Rows fetched: 100000\n",
            "Processed chunk with offset 2600000, Rows fetched: 100000\n",
            "Processed chunk with offset 2700000, Rows fetched: 100000\n",
            "Processed chunk with offset 2800000, Rows fetched: 100000\n",
            "Processed chunk with offset 2900000, Rows fetched: 100000\n",
            "Processed chunk with offset 3000000, Rows fetched: 100000\n",
            "Processed chunk with offset 3100000, Rows fetched: 100000\n",
            "Processed chunk with offset 3200000, Rows fetched: 100000\n",
            "Processed chunk with offset 3300000, Rows fetched: 100000\n",
            "Processed chunk with offset 3400000, Rows fetched: 100000\n",
            "Processed chunk with offset 3500000, Rows fetched: 100000\n",
            "Processed chunk with offset 3600000, Rows fetched: 100000\n",
            "Processed chunk with offset 3700000, Rows fetched: 100000\n",
            "Processed chunk with offset 3800000, Rows fetched: 100000\n",
            "Processed chunk with offset 3900000, Rows fetched: 100000\n"
          ]
        }
      ],
      "source": [
        "# Read in the CSV file from query_1\n",
        "data_query_1 = pd.read_csv(\"./data/processed/queried_data_step_1.csv\")\n",
        "\n",
        "# Verify key columns\n",
        "print(\"data_query_1 columns:\", data_query_1.columns)\n",
        "\n",
        "chunk_size = 100000\n",
        "data_chunks = []\n",
        "offset = 0\n",
        "\n",
        "# Adjusted query to include canonical_smiles via a join\n",
        "join_query_2 = \"\"\"\n",
        "SELECT cs.CANONICAL_SMILES, a.STANDARD_VALUE, a.STANDARD_TYPE, a.DOC_ID\n",
        "FROM ACTIVITIES a\n",
        "LEFT JOIN COMPOUND_STRUCTURES cs ON a.MOLREGNO = cs.MOLREGNO\n",
        "WHERE a.standard_value <= 1000\n",
        "LIMIT ? OFFSET ?;\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    # Fetch data in chunks\n",
        "    chunk = pd.read_sql_query(join_query_2, conn, params=(chunk_size, offset))\n",
        "    if chunk.empty:\n",
        "        break\n",
        "\n",
        "    # Merge the chunk with the CSV dataframe\n",
        "    merged_chunk = pd.merge(\n",
        "        data_query_1,\n",
        "        chunk,\n",
        "        on='CANONICAL_SMILES',\n",
        "        how='left'\n",
        "    )\n",
        "    data_chunks.append(merged_chunk)\n",
        "    offset += chunk_size\n",
        "\n",
        "    # Progress monitoring\n",
        "    print(f\"Processed chunk with offset {offset}, Rows fetched: {len(chunk)}\")\n",
        "\n",
        "# Combine all chunks\n",
        "data_query_2 = pd.concat(data_chunks, ignore_index=True)\n",
        "print(data_query_2.shape)\n",
        "print(data_query_2.head())\n",
        "\n",
        "# Save the final dataset\n",
        "data_query_2.to_csv(\"./data/processed/data_query_2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in our csv file once more\n",
        "data_query_4 = pd.read_csv(\"./data/processed/data_query_4.csv\")\n",
        "\n",
        "# No need for the doc_id column anymore, just drop now.\n",
        "data_query_4 = data_query_4.drop(columns = ['DOC_ID'])\n",
        "\n",
        "chunk_size = 100000\n",
        "offset = 0\n",
        "data_chunks = []\n",
        "\n",
        "join_query_4 = \"\"\"\n",
        "SELECT PREF_NAME, TARGET_NAME, TARGET_TYPE, TID\n",
        "FROM TARGET_DICTIONARY\n",
        "LIMIT ? OFFSET ?;\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    chunk = pd.read_sql_query(join_query_4, conn, params=(chunk_size, offset))\n",
        "    if chunk.empty:\n",
        "        break\n",
        "    \n",
        "    data_chunks.append(chunk)\n",
        "    offset += chunk_size\n",
        "    \n",
        "target_dictionary_data = pd.concat(data_chunks, ignore_index=True)\n",
        "\n",
        "data_query_5 = pd.merge(\n",
        "    data_query_4,\n",
        "    target_dictionary_data,\n",
        "    on = 'TID',\n",
        "    how = 'left'\n",
        ")\n",
        "\n",
        "print(data_query_5.head())\n",
        "print(data_query_5.shape)\n",
        "\n",
        "data_query_5.to_csv(\"./data/processed/eda_db.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 2: Add Bioactivity Data from activities table.\n",
        "\n",
        "This step will introduce the metrics standard_value and standard_type.  We will use the built-in temp table variable s1 here to just add on to our previous work.  Also note here in this query we will filter our data based on the standard_value.  This will help in processing as well as help us determine how effective a molecule is at interacting with a biological target.  For this value, lower values indicate higher potency (i.e. the molecule is more effective at lower concentrations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9955476, 13)\n",
            "   molregno                                  canonical_smiles  \\\n",
            "0         1      Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccccc1Cl   \n",
            "1         1      Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccccc1Cl   \n",
            "2         2   Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(C#N)cc1   \n",
            "3         2   Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(C#N)cc1   \n",
            "4         3  Cc1cc(-n2ncc(=O)[nH]c2=O)cc(C)c1C(O)c1ccc(Cl)cc1   \n",
            "\n",
            "            standard_inchi_key  full_mwt  alogp     psa  hba  hbd  rtb  \\\n",
            "0  OWRSAHYFSSNENM-UHFFFAOYSA-N    341.75   2.11   84.82  5.0  1.0  3.0   \n",
            "1  OWRSAHYFSSNENM-UHFFFAOYSA-N    341.75   2.11   84.82  5.0  1.0  3.0   \n",
            "2  ZJYUMURGSZQFMH-UHFFFAOYSA-N    332.32   1.33  108.61  6.0  1.0  3.0   \n",
            "3  ZJYUMURGSZQFMH-UHFFFAOYSA-N    332.32   1.33  108.61  6.0  1.0  3.0   \n",
            "4  YOMWDCALSDWFSV-UHFFFAOYSA-N    357.80   2.27   87.98  5.0  2.0  3.0   \n",
            "\n",
            "   standard_value standard_type  doc_id  parent_molregno  \n",
            "0            7.08         pH1/2    7792                1  \n",
            "1           60.00           MEC    7792                1  \n",
            "2            7.08         pH1/2    7792                2  \n",
            "3          125.00           MEC    7792                2  \n",
            "4           30.00           MEC    7792                3  \n"
          ]
        }
      ],
      "source": [
        "# Step 3.1: Add hierarchy data from molecule_hierarchy.\n",
        "data_query_2 = pd.read_csv(\"./data/processed/data_query_2.csv\")\n",
        "\n",
        "# This time the logic will be a little different since we are just doing a quick join, can do the merge after declaring.\n",
        "molecule_hierarchy = pd.read_sql_query(\"SELECT MOLREGNO, PARENT_MOLREGNO FROM MOLECULE_HIERARCHY;\", conn)\n",
        "\n",
        "# Now set up the merge on our PK 'molregno'\n",
        "data_query_3 = pd.merge(\n",
        "    data_query_2,\n",
        "    molecule_hierarchy[['MOLREGNO', 'PARENT_MOLREGNO']],\n",
        "    on = 'MOLREGNO',\n",
        "    how = 'left'\n",
        ")\n",
        "\n",
        "print(data_query_3.shape)\n",
        "print(data_query_3.head())\n",
        "\n",
        "data_query_3.to_csv(\"./data/processed/data_query_3.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now add TID from assays table as a bridge for us to get to the target_dictionary table\n",
        "data_query_3 = pd.read_csv(\"./data/processed/data_query_3.csv\")\n",
        "\n",
        "chunk_size = 100000\n",
        "offset = 0\n",
        "data_chunks = []\n",
        "\n",
        "# Query to get data from the assays table, in doc_id and tid\n",
        "join_query_3 = \"\"\"\n",
        "SELECT DOC_ID, TID\n",
        "FROM ASSAYS\n",
        "LIMIT ? OFFSET ?;\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    chunk = pd.read_sql_query(join_query_3, conn, params=(chunk_size, offset))\n",
        "\n",
        "    if chunk.empty:\n",
        "        break\n",
        "    \n",
        "    data_chunks.append(chunk)\n",
        "    offset += chunk_size\n",
        "\n",
        "assays_data = pd.concat(data_chunks, ignore_index = True)\n",
        "\n",
        "data_query_4 = pd.merge(\n",
        "    data_query_3,\n",
        "    assays_data,\n",
        "    on = 'DOC_ID',\n",
        "    how = 'left'\n",
        ")\n",
        "\n",
        "print(data_query_4.head())\n",
        "print(data_query_4.shape)\n",
        "\n",
        "data_query_4.to_csv(\"./data/processed/data_query_4.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in our csv file once more\n",
        "data_query_4 = pd.read_csv(\"./data/processed/data_query_4.csv\")\n",
        "\n",
        "# No need for the doc_id column anymore, just drop now.\n",
        "data_query_4 = data_query_4.drop(columns = ['DOC_ID'])\n",
        "\n",
        "chunk_size = 100000\n",
        "offset = 0\n",
        "data_chunks = []\n",
        "\n",
        "join_query_4 = \"\"\"\n",
        "SELECT PREF_NAME, TARGET_NAME, TARGET_TYPE, TID\n",
        "FROM TARGET_DICTIONARY\n",
        "LIMIT ? OFFSET ?;\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    chunk = pd.read_sql_query(join_query_4, conn, params=(chunk_size, offset))\n",
        "    if chunk.empty:\n",
        "        break\n",
        "    \n",
        "    data_chunks.append(chunk)\n",
        "    offset += chunk_size\n",
        "    \n",
        "target_dictionary_data = pd.concat(data_chunks, ignore_index=True)\n",
        "\n",
        "data_query_5 = pd.merge(\n",
        "    data_query_4,\n",
        "    target_dictionary_data,\n",
        "    on = 'TID',\n",
        "    how = 'left'\n",
        ")\n",
        "\n",
        "print(data_query_5.head())\n",
        "print(data_query_5.shape)\n",
        "\n",
        "data_query_5.to_csv(\"./data/processed/eda_db.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Important!  Close the connection\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 5: Connection to Azure MySQL Database to Upload Data and Data Review\n",
        "\n",
        "We finally have the data we need at this time to send to our Azure MySQL database.  We will now connect to said database and upload the acquired dataset so we can use it at will during future phases of our project.  Before doing so however we will go through it quickly and review our features and see if there are any we can prune due to being redundant to our cause.  We will look to do this before making our submission to the server and moving on to the EDA portion of the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's read in our eda_db file and review the features.\n",
        "eda_db = pd.read_csv(\"./data/processed/eda_db.csv\")\n",
        "\n",
        "print(eda_db.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to the Azure MySQL Database:\n",
        "try:\n",
        "        conn = mysql.connector.connect(\n",
        "                hostname = MYSQL_CONFIG[\"hostname\"],\n",
        "                port = MYSQL_CONFIG[\"port\"],\n",
        "                username = MYSQL_CONFIG[\"username\"],\n",
        "                password = MYSQL_CONFIG[\"password\"],\n",
        "                database = MYSQL_CONFIG[\"database\"],\n",
        "                ssl_mode = MYSQL_CONFIG[\"ssl_mode\"]\n",
        "        )\n",
        "        print(\"Connected to the Azure MySQL Database successfully!\")\n",
        "except mysql.connector.Error as e:\n",
        "        print(f\"Error connecting to the database: {e}\")\n",
        "        exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    # Need to create a new table in the database,\n",
        "    # this will provide the foundational structure for our pending data upload.\n",
        "    table_name = \"chembl_data\"\n",
        "    create_table_query = f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
        "        {', '.join([f'{col} VARCHAR(255)' for col in eda_db.columns])}\n",
        "    );\n",
        "    \"\"\"\n",
        "    cursor.execute(create_table_query)\n",
        "    print(f\"Table '{table_name} created or verified successfully!\")\n",
        "    \n",
        "    # Now we will insert and upload our data to our table we just created.\n",
        "    insert_query = f\"\"\"\n",
        "    INSERT INTO {table_name} ({', '.join(eda_db.columns)})\n",
        "    VALUES ({', '.join(['%s' for _ in eda_db.columns])});\n",
        "    \"\"\"\n",
        "    \n",
        "    cursor.exectutemany(insert_query, eda_db.values.tolist())\n",
        "    conn.commit()\n",
        "    \n",
        "    print(f\"Data uploaded successfully to table '{table_name}'.\")\n",
        "except mysql.connector.Error as e:\n",
        "    print(f\"Error uploading data: {e}\")\n",
        "finally:\n",
        "    cursor.close()\n",
        "    conn.close()\n",
        "    print(\"Database connection closed.\")\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
